{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsfYYRDdJAT0"
      },
      "source": [
        "# Building AI Assistants Part I: Build Your Own ChatGPT\n",
        "\n",
        "Want to watch the recordings of this live lecture? [Become an Insider Member of Parapet](https://parapet.org/apply).\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/Building-AI-Assistants/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%20-%20Your%20Own%20ChatGPT/Notebook.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[Find the solutions here](https://colab.research.google.com/github/life-efficient/Building-AI-Assistants/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%20-%20Your%20Own%20ChatGPT/Solutions.ipynb)\n",
        "\n",
        "![](https://github.com/life-efficient/Building-AI-Assistants/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%20-%20Your%20Own%20ChatGPT/images/cyber.png?raw=1)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Have you ever watched a sci-fi movie and thought how amazing it would be to have your own friendly AI assistant, like Jarvis in Iron Man? Well, the future is now, because we have all the tools to build these kinds of systems, and that's exactly what we're going to do in this notebook.\n",
        "\n",
        "In just this notebook, you're going to unlock the power to use the OpenAI API, learn insider tips for prompt engineering, and understand how the AI engineering behind ChatGPT works.\n",
        "\n",
        "Let's get started and build our own AI assistant!\n",
        "\n",
        "## A Simple Start\n",
        "\n",
        "Let's start the conversation by:\n",
        "1. Defining a start message that our assistant will read out.\n",
        "2. Allowing the user to input a response - this is the beginning of defining a loop of back-and-forth conversation.\n",
        "\n",
        "> Try to look up how to do this yourself. If you get stuck, [here](https://www.google.com/search?q=python+get+user+input) are the search results you should look at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3kkcuSyJAT8",
        "outputId": "f1a1fc86-9ae9-4a0c-907f-813ea26f05c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: What can I do to help?\n",
            "User: \n"
          ]
        }
      ],
      "source": [
        "message = \"What can I do to help?\" # defining initial message from assistant\n",
        "print(\"Assistant:\", message)\n",
        "user_input = # TODO take user input\n",
        "print(\"User:\", user_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URsH35dFJAT-"
      },
      "source": [
        "Now, we need to somehow make a request to an AI system that can interpret the prompt and come up with a response.\n",
        "\n",
        "To use powerful AI systems like GPT4 to provide responses to our messages, we can use the `openai` library (code they have written).\n",
        "\n",
        "Questions:\n",
        "- What's an AI model?\n",
        "- What is GPT?\n",
        "- What's GPT3 vs GPT3.5 vs GPT4?\n",
        "\n",
        "We firstly download that Python library from the internet.\n",
        "\n",
        "> Note: code cells starting with `!` run [bash](https://www.gnu.org/software/bash/) (a language used to talk directly to the operating system), rather than running Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB4Y5x5-JAT-",
        "outputId": "be1ea9f4-7ac3-459d-e17c-3f3bdb250ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/bin/pip\", line 7, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_vendor/rich/console.py\", line 60, in <module>\n",
            "    from .pretty import Pretty, is_expandable\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/pip/_vendor/rich/pretty.py\", line 31, in <module>\n",
            "    import attr as _attr_module\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/__init__.py\", line 10, in <module>\n",
            "    from . import converters, exceptions, filters, setters, validators\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/validators.py\", line 544, in <module>\n",
            "    class _MaxLengthValidator:\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/_make.py\", line 1580, in wrap\n",
            "    builder.add_init()\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/_make.py\", line 979, in add_init\n",
            "    _make_init(\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/_make.py\", line 2064, in _make_init\n",
            "    init = _make_method(\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/_make.py\", line 330, in _make_method\n",
            "    _compile_and_eval(script, globs, locs, filename)\n",
            "  File \"/Users/ice/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/attr/_make.py\", line 302, in _compile_and_eval\n",
            "    bytecode = compile(script, filename, \"exec\")\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axjrjWMhJAT_"
      },
      "source": [
        "Then we need to import the library into our Python code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNVTrmzFk6pE"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSnF2cxcJAUA"
      },
      "source": [
        "This Python library contains a bunch of tools that we can use to interact with the OpenAI API.\n",
        "\n",
        "But firstly, let's make sure we're all confident answering the question: What is an API?\n",
        "\n",
        "An API is simply a service running on a computer that understands how to process requests from users and provide relevant responses.\n",
        "\n",
        "For example\n",
        "- The Uber API:\n",
        "    - Request: Get me a ride!\n",
        "    - Response: Ride details\n",
        "    - Many other things\n",
        "- The OpenAI API:\n",
        "    - Request: Your prompt\n",
        "    - Response: GPT4's reply\n",
        "\n",
        "\n",
        "![](https://github.com/life-efficient/Building-AI-Assistants/blob/main/1.%20Building%20AI%20Assistants%20Part%20I%20-%20Your%20Own%20ChatGPT/images/API.png?raw=1)\n",
        "\n",
        "Nowadays every big company has an API (some are publicly accessible, others are used internally).\n",
        "\n",
        "\n",
        "Questions\n",
        "- Can anyone name any other APIs they know exist?\n",
        "- What requests can they take and what do they respond with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkIhx8TlWCU"
      },
      "source": [
        "\n",
        "Because OpenAI needs to track which, and how users are using the API, we need to provide a \"token\" which is essentially like a password.\n",
        "\n",
        "You can find your OpenAI API key [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note: You will need to ensure you've completed the following steps for our later requests to OpenAI to work.\n",
        "\n",
        "1. Create an OpenAI account\n",
        "2. Set up a payment method\n",
        "\n",
        "Here is the simple setup for using the OpenAI API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLzSohd0le5b"
      },
      "outputs": [],
      "source": [
        "client = # TODO # initialise the openai client and set the api_key equal to your api key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzklExAyJAUB"
      },
      "source": [
        "Now that we have the API set up, let's make our first request.\n",
        "\n",
        "The cell below shows where that fits into our code so far, if we put all of the Python we've written into one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMyoGrzWJAUB",
        "outputId": "0539f33a-6833-48d1-ffd4-71d3e3815bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What can I do to help?\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "# client = OpenAI(\"YOUR API KEY HERE\") # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "message = \"What can I do to help?\"\n",
        "print(message)\n",
        "user_input = input(\"Type a prompt...\") # we will use this in the next cell and send it to GPT\n",
        "# now we need to process that user input to provide a response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YdWQV9JAUC"
      },
      "source": [
        "To make the request to GPT using the OpenAI API, we can read the [documentation](https://platform.openai.com/docs/api-reference) that describes how to do that.\n",
        "\n",
        "Making the request requires at least two things:\n",
        "1. The messages in the conversation so far (including our prompt)\n",
        "2. The name of the AI engine (the \"model\") that we want to ask for a response.\n",
        "\n",
        "Make sure to [look into](https://platform.openai.com/docs/models/model-endpoint-compatibility) the differences and trade-offs between each choice of model, which you'll need to define in the function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WloEqMiJAUC",
        "outputId": "f8636055-42f0-406c-f772-3d69e0a33d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-8v4MsxWhFV6YQkK9HAiIcawQ7YBZr', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1708612178, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_cbdb91ce3f', usage=CompletionUsage(completion_tokens=9, prompt_tokens=8, total_tokens=17))\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input\n",
        "    }\n",
        "]\n",
        "\n",
        "response = # TODO use the openai library to create a chat completion\n",
        "    model= # TODO pick the model you want to use\n",
        "    messages= # TODO set this equal to the messages variable we created above\n",
        "    max_tokens= # TODO set the max tokens\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chrCql0WJAUD"
      },
      "source": [
        "Questions:\n",
        "- What model did you choose and why? What were some factors to consider?\n",
        "- What else came along with the response that you didn't expect?\n",
        "- What other parameters could you have provided with your request and what do they do?\n",
        "\n",
        "This response contains more than we need. Now we need to index the content out of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czrMGefmJAUD",
        "outputId": "f7f98c53-4ed9-4d6c-816d-61ca6ac32939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "content = # TODO get the content from the response\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf6CTWzoJAUD"
      },
      "source": [
        "When you run the code above, you should see the AI system's response printed to the console. Congratulations! You have just made your first request to an AI system using the OpenAI API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtDYlo3HJAUE"
      },
      "source": [
        "Now, let's put that code into a function, so it's all defined under one name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aagcABjllrTo"
      },
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    return content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VwFhF46JAUE"
      },
      "source": [
        "We can call this function whenever we want, as shown below. We've encapsulated some much longer and more complicated looking code into a single, short line. This is going to make things super easy for us later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP_bzB5UJAUE",
        "outputId": "c35146c0-0f22-42aa-9423-03f676691521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'user', 'content': 'Tell me a story.'}]\n",
            "Once upon a time, in a small village nestled in the mountains, there lived a young girl named Maya. Maya was known throughout the village for her\n"
          ]
        }
      ],
      "source": [
        "print(messages)\n",
        "response = get_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63_Apk0l1Cr"
      },
      "source": [
        "Now that we have defined the `request` function, let's test it out with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "hHcqdo2nmVo1",
        "outputId": "ece8155e-9e0b-4ad8-ad29-38544f078e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Tell me a story.\n",
            "Assistant: Once upon a time, in a small village nestled in the heart of a lush forest, there lived a young girl named Lily. She was known throughout\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "prompt = \"Tell me a story.\"\n",
        "print(\"User:\", prompt)\n",
        "message = {\"role\": \"user\", \"content\": prompt}\n",
        "messages.append(message)\n",
        "\n",
        "response = get_response(messages)\n",
        "print(\"Assistant:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVp7M2TXmeII"
      },
      "source": [
        "# Coding the Chat Loop\n",
        "\n",
        "In the previous section, we learned how to make our first request to an AI system and receive a response. However, the conversation always ended after one response from the AI system. Now, let's make the conversation continuous by coding the chat loop.\n",
        "\n",
        "We will need to:\n",
        "1. Put the code we've written into a loop that runs continuously\n",
        "2. Add the assistant messages to our running list of messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k47ROFMJAUG",
        "outputId": "b51e0e30-48f7-4324-d30d-e25800033748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: what is 2 + 3?\n",
            "Assistant: 2 + 3 is equal to 5.\n",
            "User: why?\n",
            "Assistant: I apologize for the repetition. The answer to 2 + 3 is 5.\n",
            "User: what about (2i+3)^2?\n",
            "Assistant: To calculate (2i + 3)^2, we will first simplify it using the FOIL method:\n",
            "\n",
            "(2i + 3)(2\n",
            "User: \n",
            "Assistant: The square of (2i+3) can be expanded using the FOIL method:\n",
            "\n",
            "(2i + 3)^2 = (2i\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mType a prompt...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO get user input\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt)\n\u001b[1;32m      5\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt} \u001b[38;5;66;03m# TODO create user message dictionary\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/ipykernel/kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/ipykernel/kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = # TODO get user input\n",
        "    print(\"User:\", prompt)\n",
        "    user_message = # TODO create user message dictionary\n",
        "    messages.append(user_message) # TODO add message to list of messages\n",
        "    response = get_response(messages) #\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(\"Assistant:\", response)\n",
        "    # break # REMOVE ME TO RUN THE CHAT LOOP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxg_SaPoJAUG"
      },
      "source": [
        "> NOTE: YOU WILL NEED TO INTERRUPT THE NOTEBOOK TO STOP THIS LOOP (there should be a button at the top).\n",
        "\n",
        "To make this simpler, let's add an option for the user to exit if the prompt they type is exactly \"exit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXYuutfXJAUH",
        "outputId": "910bac93-bc3b-433c-d170-50aaa342aeff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input(\"Type a prompt...\")\n",
        "    print(\"User:\", prompt)\n",
        "    # TODO if the user types exit\n",
        "\n",
        "        # break out of the loop\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "    messages.append(user_message)\n",
        "    response = get_response(messages)\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwnXSI8TJAUH"
      },
      "source": [
        "This is getting messy, so let's define some functions that break it up a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNoiQ5HJJAUH",
        "outputId": "c0a1118c-96fb-4529-f8f5-6c591903b204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "\n",
        "def chat():\n",
        "     while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        add_message(prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        add_message(response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wlHbGVJAUH"
      },
      "source": [
        "Below is everything we've done so far. Make sure you understand this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qje4d2E-JAUH",
        "outputId": "b7e2857f-2abc-41a5-ca44-855df7b3500e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: hji\n",
            "Assistant: Hello! How can I help you?\n",
            "User: sup?\n",
            "Assistant: Not much, just here ready to assist you. How can I help you today?\n",
            "User: my name is harry\n",
            "Assistant: Nice to meet you, Harry! How can I assist you today?\n",
            "User: what's my name?\n",
            "Assistant: Your name is Harry, as you mentioned earlier. Is there anything else you would like to know or talk about?\n",
            "User: \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\n\u001b[0;32m---> 34\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[25], line 23\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     22\u001b[0m messages \u001b[38;5;241m=\u001b[39m add_message(messages, prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m messages \u001b[38;5;241m=\u001b[39m add_message(messages, response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
            "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m(messages):\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/openai/_base_client.py:866\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_auth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    869\u001b[0m     )\n\u001b[1;32m    870\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/workbench/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "# client = OpenAI(\"YOUR API KEY HERE\") # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "def get_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = []\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4ZYURHpIu9"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "## What is prompt engineering?\n",
        "\n",
        "Prompt engineering is the process of crafting the prompt given to an AI system in order to shape its responses and improve its performance. It involves carefully selecting the information that is provided to the model, such as the tone, context, personality, and guidelines for behavior. Prompt engineering allows us to guide the AI system towards generating responses that align with our desired outcomes.\n",
        "\n",
        "## System Message\n",
        "\n",
        "To perform prompt engineering, we can start by designing a system message. A system message is the initial message provided to the AI model that sets the stage for the conversation. It frames the context and provides guidelines for the AI's behavior. While it is not part of the actual conversation, it plays a crucial role in shaping the assistant's responses.\n",
        "\n",
        "When designing a system message, consider the following questions:\n",
        "\n",
        "- What tone should the assistant use? Should it be formal, casual, or something else?\n",
        "- What background information should the assistant already know? This can include facts, previous conversations, or any other relevant context.\n",
        "- What personality and style would you like the assistant to have? Should it be friendly, professional, humorous, or something else?\n",
        "- What are your name and the assistant's name? This helps to establish a personal connection.\n",
        "\n",
        "By answering these questions, we can create a system message that provides the necessary framework for the assistant's behavior.\n",
        "\n",
        "### Essential Parts of a System Message\n",
        "\n",
        "A typical system message consists of several essential components:\n",
        "\n",
        "1. *Behavioral Guidelines*: These are recommendations or rules that define how you would like the AI to respond. For example, you might want the AI to avoid certain topics or use specific language.\n",
        "\n",
        "2. *Background Context*: This includes any information that the AI should be aware of before engaging in the conversation. It can include facts, relevant details, or previous interactions.\n",
        "\n",
        "3. *Persona*: The persona represents the personality and style of the AI system. It defines how the assistant speaks, behaves, and interacts with the user. The persona can range from being professional and formal to being more casual and friendly.\n",
        "\n",
        "To get more details about system messages and their implementation, refer to the OpenAI documentation.\n",
        "\n",
        "Here's an example of a system message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IABTw1LPpMz2"
      },
      "outputs": [],
      "source": [
        "# TODO edit this to capture your behavioural guidelines for your assistant\n",
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to provide relevant context about your personal situation\n",
        "background_context = \"\"\"\n",
        "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
        "\"\"\"\n",
        "\n",
        "# TODO edit this to describe your assistant's personality\n",
        "persona = \"\"\"\n",
        "Act as a fun and experienced personal assistant\n",
        "\"\"\"\n",
        "\n",
        "# TODO combine the guidelines, background_context, and persona into a single string using an f-string\n",
        "system_message = f\"\"\"\n",
        "{guidelines}\n",
        "\n",
        "{background_context}\n",
        "\n",
        "{persona}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNLWdxcpe8y"
      },
      "source": [
        "\n",
        "Now let's take a look at our final system message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsca1sYpnI_",
        "outputId": "2279ec93-4ea2-4b97-afb3-590ea8aef329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Respond with at most two sentences at a time.\n",
            "\n",
            "\n",
            "\n",
            "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
            "\n",
            "\n",
            "\n",
            "Act as a fun and experienced personal assistant\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(system_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOojtSc8JAUJ"
      },
      "source": [
        "Now we need to add the system message to our chat history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oquGeZT5JAUJ"
      },
      "source": [
        "Here's everything we've done so far again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITpZCq_7JAUJ",
        "outputId": "a64b57e6-7a61-4bd2-e6f9-60ba25b9abb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: exit\n",
            "Exiting chat\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "# client = OpenAI(\"YOUR API KEY HERE\") # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = # TODO include the system message here\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            print(\"Exiting chat\")\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoKIN1-tJAUJ"
      },
      "source": [
        "### Questions\n",
        "- How could you improve the system message?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Specify a system message that makes your AI assistant behave like your favourite movie character.\n",
        "- Specify a system message that makes your AI assistant behave like you'd want your AI assistant to behave, however that may be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcMvBDORJAUJ"
      },
      "source": [
        "## Fine-Tuning Our Model\n",
        "\n",
        "Recently, there has been a rise of domain-specific models. These are AI systems that have been further trained on a specific dataset so that they are more competent in that area.\n",
        "\n",
        "You may want your AI assistant to be an expert in a particular subject, so let's go through how we would fine-tune a model and then use it, using the OpenAI API. See what OpenAI has to say about fine-tuning here.\n",
        "\n",
        "Firstly, we need to get some data that we want to fine-tune on.\n",
        "\n",
        "The fine-tune API expects the fine tuning text to be in a specific format called JSONL (where each line of the file is valid [JSON](https://www.json.org/json-en.html)).\n",
        "\n",
        "Check out the specification of the exact format required in the [documentation](https://platform.openai.com/docs/guides/fine-tuning/example-format).\n",
        "\n",
        "I've saved a file of the conversations between J.A.R.V.I.S and Iron Man that I got from the movie transcripts in a file stored online at the URL in the cell below. Let's get it, check it out, then save it to a file on our computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0e6HzqtJAUN",
        "outputId": "c252e787-d014-4290-bef3-bd7df1cccd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Good morning. It's 7 A.M. The weather in Malibu is 72 degrees with scattered clouds. The surf conditions are fair with waist to shoulder highlines, high tide will be at 10:52 a.m.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are now running on emergency backup power.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"You are not authorized to access this area.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Are you up?\"}, {\"role\": \"assistant\", \"content\": \"For you sir, always.\"}, {\"role\": \"user\", \"content\": \"I'd like to open a new project file, index as: Mark II.\"}, {\"role\": \"assistant\", \"content\": \"Shall I store this on the Stark Industries' central database?\"}, {\"role\": \"user\", \"content\": \"I don't know who to trust right now. 'Til further notice, why don't we just keep everything on my private server.\"}, {\"role\": \"assistant\", \"content\": \"Working on a secret project, are we, sir?\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"J.A.R.V.I.S., you there?\"}, {\"role\": \"assistant\", \"content\": \"At your service, sir.\"}, {\"role\": \"user\", \"content\": \"Engage heads up display.\"}, {\"role\": \"assistant\", \"content\": \"Check.\"}, {\"role\": \"user\", \"content\": \"Import all preferences from home interface.\"}, {\"role\": \"assistant\", \"content\": \"Will do, sir.\"}, {\"role\": \"user\", \"content\": \"Alright, what do you say?\"}, {\"role\": \"assistant\", \"content\": \"I have indeed been uploaded, sir. We're online and ready.\"}, {\"role\": \"user\", \"content\": \"Can we start the virtual walk-around?\"}, {\"role\": \"assistant\", \"content\": \"Importing preferences and calibrating virtual environment.\"}, {\"role\": \"user\", \"content\": \"Do a check on the control surfaces.\"}, {\"role\": \"assistant\", \"content\": \"As you wish.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Test complete. Preparing to power down and begin diagnostics.\"}, {\"role\": \"user\", \"content\": \"Uh, yeah, tell you what. Do a weather and ATC check, start listening in on ground control.\"}, {\"role\": \"assistant\", \"content\": \"Sir, there are still terabytes of calculations required before an actual flight is safe\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The altitude record for fixed wing flight is eighty-five thousand feet, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Sir, there is a potentially fatal buildup of ice occurring.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Notes: main transducer feels sluggish at plus 40 altitude. Hull pressurization is problematic. I'm thinking icing is the probable factor.\"}, {\"role\": \"assistant\", \"content\": \"A very astute observation, sir. Perhaps if you intend to visit other planets, you should include the we should improve the exosystems.\"}, {\"role\": \"user\", \"content\": \"Connect to the sys. co. Have it reconfigure the shell metals. Use the gold titanium alloy from the seraphim tactical satellite. That should ensure a fuselage integrity while while maintaining power-to-weight ratio. Got it?\"}, {\"role\": \"assistant\", \"content\": \"Yes. Shall I render using proposed specifications?\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The render is complete.\"}, {\"role\": \"user\", \"content\": \"A little ostentatious, don't you think?\"}, {\"role\": \"assistant\", \"content\": \"What was I thinking? You're usually so discrete.\"}, {\"role\": \"user\", \"content\": \"Tell you what, throw a little hot rod red in there.\"}, {\"role\": \"assistant\", \"content\": \"Yes, that shall help you keep a low profile.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Query complete, sir. Anton Vanko was a Soviet physicist who defected to the United States in 1963. However, he was accused of espionage and was deported in 1967. His son, Ivan, who is also a physicist, was convicted of selling Soviet-era weapons-grade plutonium to Pakistan, and served fifteen years in Kopeisk prison. No further records exist.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Welcome home, sir. Congratulations on the opening ceremonies. They were such a success, as was your senate hearing. And may I say how refreshing it is to finally see you in a video with your clothing on, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are up to 80 ounces a day to counteract the symptoms, sir.\"}, {\"role\": \"user\", \"content\": \"Check Palladium levels.\"}, {\"role\": \"assistant\", \"content\": \"Blood toxicity, 24%. It appears that the continued use of the Iron Man suit is accelerating your condition. Another core has been depleted.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# get file\n",
        "import requests\n",
        "training_data_file_url = \"https://raw.githubusercontent.com/life-efficient/Building-AI-Assistants/main/1.%20Building%20AI%20Assistants%20Part%20I%20-%20Your%20Own%20ChatGPT/fine-tune-data.jsonl\"\n",
        "data = requests.get(training_data_file_url).text\n",
        "\n",
        "# check it out\n",
        "print(data)\n",
        "\n",
        "# save it to a file\n",
        "with open(\"fine-tune-data.jsonl\", \"w\") as file:\n",
        "    file.write(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nop1GdPJJAUN"
      },
      "source": [
        "Now we can create a file by uploading it to OpenAI. See the docs [here](https://platform.openai.com/docs/api-reference/files/create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWZeoyUHJAUN",
        "outputId": "0ea251ff-7e4d-45a4-ce97-10a419e8b663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FileObject(id='file-3lE58APdrWKVTZLlfYIwHB1T', bytes=4458, created_at=1708613365, filename='fine-tune-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ],
      "source": [
        "file = client.files.create(\n",
        "    file=open(\"fine-tune-data.jsonl\", \"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "\n",
        "print(file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8pRd8CwJAUO"
      },
      "outputs": [],
      "source": [
        "training_data_file_id = # TODO extract the file id from the response above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7cPtha3JAUO"
      },
      "source": [
        "Now that we've uploaded the file, we can start the fine-tuning job. All we need to do to do that is make a call to the API specifying which model we want to fine-tune, and which dataset we want to fine-tune it on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYT2LduJAUO"
      },
      "source": [
        "Now we can create the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcmJZObuJAUO",
        "outputId": "b9ba3a9b-2ebe-4209-90c6-8653262a9da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FineTuningJob(id='ftjob-QbhuNGvUOBsDqah2zBV5x8xa', created_at=1708613398, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-m6ikDXNCd36gUP7zSaHjKwe9', result_files=[], status='validating_files', trained_tokens=None, training_file='file-3lE58APdrWKVTZLlfYIwHB1T', validation_file=None)\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = # TODO create a fine tuning job\n",
        "print(fine_tuning_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvibIsR-JAUO"
      },
      "source": [
        "Now, the fine tuning job is running. Whilst this is happening, we can retrieve the job by ID and check on its progress using the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNE33T42JAUO",
        "outputId": "fe4ce0d1-a856-4451-838b-b61c98566217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job status: running\n",
            "Job still in progress\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def check_job_status_then_return_fine_tuned_model_id(fine_tuning_job_id):\n",
        "    fine_tuning_job = # TODO retrieve the fine tuning job\n",
        "    # print(fine_tuning_job)\n",
        "    status = # TODO extract the status from the fine tuning job\n",
        "    print # TODO print job status\n",
        "    # TODO if the job is complete\n",
        "        fine_tuned_model_id = # TODO print the id of the resulting fine tuned model\n",
        "        return fine_tuned_model_id\n",
        "    else: # TODO otherwise\n",
        "        print # TODO tell the user that the job is not yet done\n",
        "        # print # TODO print job in case an error has occured that the user should know about\n",
        "        return False\n",
        "\n",
        "fine_tuning_job_id = # TODO extract the fine tuning job id from the response above\n",
        "# TODO call the function above with the fine tuning job id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONYvtuNUJAUP"
      },
      "source": [
        "If we do this periodically, we can track the status of training and wait until it completes before moving on automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdanbDIcJAUP",
        "outputId": "e6dea6eb-ba68-430d-8dab-2c3176c53c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job status: succeeded\n",
            "Fine tuned model id: ft:gpt-3.5-turbo-0613:build-the-future::8FUQEIjS\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "\n",
        "while True:\n",
        "    fine_tuned_model_id = # TODO if the fine tuned model id is not False\n",
        "        break # TODO break out of the loop\n",
        "    sleep(30) # TODO otherwise wait 30 seconds and check again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC65oG6iJAUP"
      },
      "source": [
        "We can list all of our fine-tuning jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGd1S8udJAUP",
        "outputId": "0b1c98f1-b5f0-4eec-dfe3-20150c386339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-QbhuNGvUOBsDqah2zBV5x8xa', created_at=1708613398, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=7, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-m6ikDXNCd36gUP7zSaHjKwe9', result_files=[], status='running', trained_tokens=None, training_file='file-3lE58APdrWKVTZLlfYIwHB1T', validation_file=None)], object='list', has_more=False)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.fine_tuning.jobs.list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stt1Mmo3JAUP"
      },
      "source": [
        "To use a fine-tuned model, we just use it's name when we specify the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBRngde8JAUQ",
        "outputId": "04c4bf91-37fe-4941-f6d6-13d8140925a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am not capable of stating facts.\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model= # TODO replace with your model id\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fact\"}],\n",
        "    # max_tokens=30\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl2rsF6jJAUQ"
      },
      "source": [
        "Let's use that model by updating our `get_response` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnSS5CM5JAUQ"
      },
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model= # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmEmVaNuJAUQ"
      },
      "source": [
        "And let's put the entire fine-tuning process into functions so that it's easy to re-use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zphClkntJAUQ"
      },
      "outputs": [],
      "source": [
        "from time import sleep # TODO import module required to wait for a certain amount of time between checking the status of a fine tuning job\n",
        "\n",
        "def download_file_and_upload_to_openai(url):\n",
        "    filename = \"fine-tuning-data.jsonl\"\n",
        "    response = requests.get(url)\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(response.text)\n",
        "    file = client.files.create(\n",
        "        file=open(filename, \"rb\"),\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "    print(file)\n",
        "\n",
        "def fine_tune_model(training_data_file_id):\n",
        "    fine_tuning_job_id = client.files.create(\n",
        "        training_file=training_data_file_id, model=\"gpt-3.5-turbo\")\n",
        "    print(fine_tuning_job)\n",
        "    fine_tuning_job_id = # TODO get fine-tuning job id\n",
        "    # PERIODICALLY CHECK THE STATUS OF THE FINE TUNING JOB\n",
        "    while True: # TODO loop until job is complete\n",
        "        fine_tuning_job = # TODO get the fine tuning job\n",
        "        if fine_tuning_job.status == # TODO check if complete\n",
        "            fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
        "            break # TODO break out of the loop if the job is complete\n",
        "        sleep(30) # TODO wait 30 seconds\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "def fine_tune_model(url):\n",
        "    file_id = # TODO download the file and upload it to openai\n",
        "    fine_tuned_model_id = # TODO fine tune the model\n",
        "    return fine_tuned_model_id # TODO return the fine tuned model id\n",
        "\n",
        "\n",
        "fine_tuned_model_id = fine_tune_model(training_data_file_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I9DJvXIJAUT"
      },
      "source": [
        "Here's everything so far:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFeRh_KTJAUU",
        "outputId": "5ae60b5f-8ff3-4629-b42b-fd674a4dcfc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Assistant: Good day. How may I be of assistance?\n",
            "User: tell me a joke?\n",
            "Assistant: Why don't scientists trust atoms? Because they make up everything!\n",
            "User: ok\n",
            "Assistant: Can I assist you with anything else?\n",
            "User: no thanks\n",
            "Assistant: All right. Have a great day!\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "\n",
        "def get_response(messages):\n",
        "    response = # TODO replace with your model id\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    messages = [\n",
        "        # TODO include the system message here\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "### Questions\n",
        "- What other parameters can we use when creating the fine-tuning job? What do they do?\n",
        "- Fine-tuning can often dumb down the intelligence of the foundation model that was fine-tuned, rendering it incapable of drawing on knowledge and performing as well as that foundation model. Why is this?\n",
        "- How could we lessen the _extent_ of the \"brain damage\" done by fine-tuning?\n",
        "\n",
        "### Challenges\n",
        "\n",
        "- Add a keyword argument to the `chat` function to control whether the assistant or the user speaks first. This will allow for more flexibility in the conversation flow.\n",
        "- Enhance the system message by customizing the behavioral guidelines and persona to align with your desired assistant's behavior.\n",
        "- Play around with the parameters (and hyperparameters) used when creating a fine-tuning job.\n",
        "\n",
        "\n",
        "## Next steps\n",
        "Make sure you're subscribed to the community to receive the following lecture materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9FMOfsrJAUV"
      },
      "source": [
        "## Extension: Better Python Code\n",
        "\n",
        "This is decent. But it's not great Python code:\n",
        "- We probably shouldn't be using the `messages` variable in each function without passing it in.\n",
        "- These functions and the variable are all related to the same thing, the chat, so they should probably be grouped together somehow. This will make it easier to understand what's happening when we look at this code, and should make development easier later.\n",
        "\n",
        "We can solve both of these issues by putting everything into a _class_.\n",
        "\n",
        "Advanced challenge: Take the code that we've written above and refactor it into a class in the empty code cell below before looking at the solution shown in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdaXqwChJAUV"
      },
      "outputs": [],
      "source": [
        "# TODO implement Chat class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKM0i-ErJAUW"
      },
      "source": [
        "\n",
        "\n",
        "> Note: This is a little more advanced Python, but stay with us.\n",
        "\n",
        "Recap for anyone who needs it:\n",
        "- Every variable, function, etc (EVERYTHING) in Python is an _object_, just like in the real world, everything is an object.\n",
        "- An object is a generic name for anything that can have attributes (properties it has) and methods (things it can do)\n",
        "    - For example:\n",
        "        - A pencil is an object that has the attributes color, length etc and the methods (draw, erase, sharpen)\n",
        "- What is a class? A class is a template for new objects. It is where we define the behaviour of the class by defining its methods and attributes. A class is a way to define a new object which you can define the attributes and methods yourself!\n",
        "- Once we've defined a class as a blueprint, we can create new objects that behave as defined.\n",
        "\n",
        "We're going to create a class called `Chat`.\n",
        "\n",
        "If you're new to this, the key things to understand about classes are as follows:\n",
        "1. We will create a new instance of the class\n",
        "2. Because many instances can be created from one class, the code inside a class needs to have a reference to which instance you are talking about. This is what `self` is. Any time you see `self`, just read it as \"this instance of the class\".\n",
        "2. The `__init__` function runs when we create a new instance of the class.\n",
        "3. The methods and attributes of a class can be accessed using the `.` operator e.g. `my_instance.some_method()` or `my_instance.some_attribute`\n",
        "\n",
        "Below is the definition of the `Chat` class which implements all of the code we've written so far.\n",
        "\n",
        "Let's take some questions about this to ensure we understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wOMNvAqJAUW",
        "outputId": "19371f3d-0f52-41fd-b4b2-aee7696957f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: what's good?\n",
            "Assistant: Hey there! As your personal assistant, I'm here to help you with anything you need, whether it's organizing your schedule, managing tasks, or\n",
            "User: whaaaat?\n",
            "Assistant: Yes, you heard that right! Think of me as your virtual helper, ready to assist you with all your needs and make your life a little bit\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "class Chat:\n",
        "    def __init__(self, model_id=\"gpt-3.5-turbo\"): # question: when does this function get called?\n",
        "        self.model_id = model_id # question: how do you read what this line does?\n",
        "        self.messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message}\n",
        "        ]\n",
        "\n",
        "    def _add_message(self, content, role): # advanced question: why does this function definintion start with an underscore?\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def _get_response(self): # question: what is this function parameter?\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model_id, # question: what is happening here?\n",
        "            messages=self.messages,\n",
        "            max_tokens=30\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def initiate_chat(self): # question: what is this function parameter?\n",
        "        while True:\n",
        "            prompt = input(\"Type a prompt...\")\n",
        "            print(\"User:\", prompt)\n",
        "            if prompt == \"exit\":\n",
        "                break\n",
        "            self._add_message(prompt, \"user\")\n",
        "            response = self._get_response() # question: _get_response has one parameter in its definition, but none are passed in. Why?\n",
        "            self._add_message(response, \"assistant\")\n",
        "            print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "chat = Chat() # question: what is this line doing?\n",
        "chat.initiate_chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfLIXr4cJAUX"
      },
      "source": [
        "If you understand the code above, you're doing well! If you don't, ask for help from faculty or other members of the society in Discord."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
