{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RT3IysHWffn"
      },
      "source": [
        "# Building AI Assistants Part I: Build Your Own ChatGPT\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Have you ever watched a sci-fi movie and thought how amazing it would be to have your own friendly AI assistant, like Jarvis in Iron Man? Well, the future is now, because we have all the tools to build these kinds of systems, and that's exactly what we're going to do in this notebook.\n",
        "\n",
        "In just this notebook, you're going to unlock the power to use the OpenAI API, learn insider tips for prompt engineering, and understand how the AI engineering behind ChatGPT works.\n",
        "\n",
        "Let's get started and build our own AI assistant!\n",
        "\n",
        "## A simple start\n",
        "\n",
        "Let's start the conversation by defining a start message that our assistant will read out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = \"What can I do to help?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's allow the user to input a response - this is the beginning of defining a loop of back-and-forth conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM8wcrzglmp3"
      },
      "source": [
        "Now let's start building up the code that will allow us to have a conversation with our AI assistant. \n",
        "\n",
        "Let's start by getting user input.\n",
        "\n",
        "> Try to look up how to do this yourself. If you get stuck, [here](https://www.google.com/search?q=python+get+user+input) are the search results you should look at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User input: hello\n"
          ]
        }
      ],
      "source": [
        "user_input = input(\"Type a prompt...\") # TODO\n",
        "print(\"User input:\", user_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we need to somehow make a request to an AI system that can interpret the prompt and come up with a response.\n",
        "\n",
        "To use powerful AI systems like GPT4 to provide responses to our messages, we can use the `openai` library (code they have written).\n",
        "\n",
        "We firstly download that from the internet.\n",
        "\n",
        "> Note: code cells starting with `!` run [bash](https://www.gnu.org/software/bash/) (a language used to talk directly to the operating system), rather than running Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (0.28.1)\n",
            "Requirement already satisfied: tqdm in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to import the library into our Python code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "xNVTrmzFk6pE",
        "outputId": "e4b40193-c507-4642-ee5a-148f0f9283ea"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python library contains a bunch of tools that we can use to interact with the OpenAI API.\n",
        "\n",
        "But firstly, let's make sure we're all confident answering the question: What is an API?\n",
        "\n",
        "An API is simply a service running on a computer that understands how to process requests from users and provide relevant responses.\n",
        "\n",
        "For example\n",
        "- The Uber API:\n",
        "    - Request: Get me a ride!\n",
        "    - Response: Ride details\n",
        "    - Many other things\n",
        "- The OpenAI API:\n",
        "    - Request: Your prompt\n",
        "    - Response: GPT4's reply\n",
        "\n",
        "Nowadays every big company has an API (some are publicly accessible, others are used internally)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkIhx8TlWCU"
      },
      "source": [
        "\n",
        "Because OpenAI needs to track which, and how users are using the API, we need to provide a \"token\" which is essentially like a password.\n",
        "\n",
        "You can find your OpenAI API key [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note: You will need to ensure you've completed the following steps for our later requests to OpenAI to work.\n",
        "\n",
        "1. Create an OpenAI account\n",
        "2. Set up a payment method\n",
        "\n",
        "Here is the simple setup for using the OpenAI API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "iLzSohd0le5b",
        "outputId": "eaac69cf-351c-470e-801c-f4bde8a0e6b5"
      },
      "outputs": [],
      "source": [
        "# TODO # get the openai library's api_key attribute and set it equal to your api key\n",
        "openai.api_key = \"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\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the API set up, let's make our first request.\n",
        "\n",
        "The cell below shows where that fits into our code so far, if we put all of the Python we've written into one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "message = \"What can I do to help?\"\n",
        "user_input = input(\"Type a prompt...\") # we will use this in the next cell and send it to GPT\n",
        "# now we need to process that user input to provide a response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make the request to GPT using the OpenAI API, we can read the [documentation](https://platform.openai.com/docs/api-reference) that describes how to do that.\n",
        "\n",
        "Making the request requires at least two things:\n",
        "1. The messages in the conversation so far (including our prompt)\n",
        "2. The name of the AI engine (the \"model\") that we want to ask for a response.\n",
        "\n",
        "Make sure to [look into](https://platform.openai.com/docs/models/model-endpoint-compatibility) the differences and trade-offs between each choice of model, which you'll need to define in the function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8Ak8wYdp0ebRLIYwhwdNf1G6zBBE0\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1697571346,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"What would you like to test?\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 8,\n",
            "    \"completion_tokens\": 7,\n",
            "    \"total_tokens\": 15\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# TODO create a dictionary called message that has two keys: role  (see documentation) and content\n",
        "message = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": user_input \n",
        "}\n",
        "\n",
        "messages = [message] # TODO create a list of messages that includes the message variable we created above\n",
        "\n",
        "response = openai.ChatCompletion.create( # TODO use the openai library to create a chat completion\n",
        "    model=\"gpt-3.5-turbo\", # TODO pick the model you want to use\n",
        "    messages=messages, # TODO set this equal to the messages variable we created above\n",
        "    max_tokens=30 # TODO set the max tokens \n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This response contains more than we need. Now we need to index the content out of it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What would you like to test?\n"
          ]
        }
      ],
      "source": [
        "content = response[\"choices\"][0][\"message\"][\"content\"] # TODO get the content from the response\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run the code above, you should see the AI system's response printed to the console. Congratulations! You have just made your first request to an AI system using the OpenAI API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's put that code into a function, so it's all defined under one name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aagcABjllrTo"
      },
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "\n",
        "\n",
        "    response = openai.ChatCompletion.create(  # TODO use the openai library to create a chat completion\n",
        "        model=\"gpt-3.5-turbo\",  # TODO pick the model you want to use\n",
        "        messages=messages,  # TODO set this equal to the messages variable we created above\n",
        "        max_tokens=30  # TODO set the max tokens\n",
        "    )\n",
        "\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    return content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call this function whenever we want, as shown below. We've encapsulated some much longer and more complicated looking code into a single, short line. This is going to make things super easy for us later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'user', 'content': 'testing'}]\n",
            "I am an AI assistant, so I am constantly being tested and evaluated to ensure that I am providing accurate and helpful information. Is there something specific you\n"
          ]
        }
      ],
      "source": [
        "print(messages)\n",
        "response = get_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63_Apk0l1Cr"
      },
      "source": [
        "Now that we have defined the `request` function, let's test it out with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "hHcqdo2nmVo1",
        "outputId": "ece8155e-9e0b-4ad8-ad29-38544f078e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time in a quaint little village named Willowbrook, there lived a kind-hearted girl named Lily. She had a heart full of dreams,\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "prompt = \"Tell me a story.\"\n",
        "message = {\"role\": \"user\", \"content\": prompt}\n",
        "messages.append(message)\n",
        "\n",
        "response = get_response(messages)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVp7M2TXmeII"
      },
      "source": [
        "# Coding the Chat Loop\n",
        "\n",
        "In the previous section, we learned how to make our first request to an AI system and receive a response. However, the conversation always ended after one response from the AI system. Now, let's make the conversation continuous by coding the chat loop.\n",
        "\n",
        "We will need to:\n",
        "1. Put the code we've written into a loop that runs continuously\n",
        "2. Add the assistant messages to our running list of messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input(\"Type a prompt...\") # TODO get user input\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt} # TODO create user message dictionary\n",
        "    messages.append(user_message) # TODO add message to list of messages\n",
        "    response = get_response(messages) # \n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(response)\n",
        "    break # REMOVE ME TO RUN THE CHAT LOOP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: YOU WILL NEED TO INTERRUPT THE NOTEBOOK TO STOP THIS LOOP (there should be a button at the top).\n",
        "\n",
        "To make this simpler, let's add an option for the user to exit if the prompt they type is exactly \"exit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = []\n",
        "while True:\n",
        "    prompt = input(\"Type a prompt...\")\n",
        "    if prompt == \"exit\": # TODO if the user types exit\n",
        "        break # break out of the loop\n",
        "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "    messages.append(user_message)\n",
        "    response = get_response(messages)\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": prompt}\n",
        "    messages.append(assistant_message)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is getting messy, so let's define some functions that break it up a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = []\n",
        "\n",
        "def chat():\n",
        "     while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        add_message(prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        add_message(response, \"assistant\")\n",
        "        print(response)\n",
        "\n",
        "def add_message(content, role):\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "\n",
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is decent. But it's not great Python code: \n",
        "- We probably shouldn't be using the `messages` variable in each function without passing it in.\n",
        "- These functions and the variable are all related to the same thing, the chat, so they should probably be grouped together somehow. This will make it easier to understand what's happening when we look at this code, and should make development easier later.\n",
        "\n",
        "We can solve both of these issues by putting everything into a _class_. \n",
        "\n",
        "Advanced challenge: Take the code that we've written above and refactor it into a class in the empty code cell below before looking at the solution shown in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO implement Chat class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "> Note: This is a little more advanced Python, but stay with us.\n",
        "\n",
        "Recap for anyone who needs it:\n",
        "- Every variable, function, etc (EVERYTHING) in Python is an _object_, just like in the real world, everything is an object.\n",
        "- An object is a generic name for anything that can have attributes (properties it has) and methods (things it can do)\n",
        "    - For example:\n",
        "        - A pencil is an object that has the attributes color, length etc and the methods (draw, erase, sharpen)\n",
        "- What is a class? A class is a template for new objects. It is where we define the behaviour of the class by defining its methods and attributes. A class is a way to define a new object which you can define the attributes and methods yourself!\n",
        "- Once we've defined a class as a blueprint, we can create new objects that behave as defined.\n",
        "\n",
        "We're going to create a class called `Chat`.\n",
        "\n",
        "If you're new to this, the key things to understand about classes are as follows:\n",
        "1. We will create a new instance of the class\n",
        "2. Because many instances can be created from one class, the code inside a class needs to have a reference to which instance you are talking about. This is what `self` is. Any time you see `self`, just read it as \"this instance of the class\".\n",
        "2. The `__init__` function runs when we create a new instance of the class.\n",
        "3. The methods and attributes of a class can be accessed using the `.` operator e.g. `my_instance.some_method()` or `my_instance.some_attribute`\n",
        "\n",
        "Below is the definition of the `Chat` class which implements all of the code we've written so far.\n",
        "\n",
        "Let's take some questions about this to ensure we understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n",
            "This is a test response.\n",
            "what does that mean?\n",
            "When someone says \"test,\" it typically means that they are conducting an examination or evaluation of something or someone to determine its accuracy, performance, or knowledge\n",
            "exit\n"
          ]
        }
      ],
      "source": [
        "class Chat:\n",
        "    def __init__(self): # question: when does this function get called?\n",
        "        self.messages = [] # question: how do you read what this line does?\n",
        "\n",
        "    def _add_message(self, content, role): # advanced question: why does this function definintion start with an underscore?\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def _get_response(self): # question: what is this function parameter?\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  \n",
        "            messages=self.messages,\n",
        "            max_tokens=30\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def initiate_chat(self): # question: what is this function parameter?\n",
        "        while True:\n",
        "            prompt = input(\"Type a prompt...\")\n",
        "            print(prompt)\n",
        "            if prompt == \"exit\":\n",
        "                break\n",
        "            self._add_message(prompt, \"user\")\n",
        "            response = self._get_response() # question: _get_response has one parameter in its definition, but none are passed in. Why?\n",
        "            print(response)\n",
        "\n",
        "\n",
        "\n",
        "chat = Chat() # question: what is this line doing?\n",
        "chat.initiate_chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4ZYURHpIu9"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "## What is prompt engineering?\n",
        "\n",
        "Prompt engineering is the process of crafting the prompt given to an AI system in order to shape its responses and improve its performance. It involves carefully selecting the information that is provided to the model, such as the tone, context, personality, and guidelines for behavior. Prompt engineering allows us to guide the AI system towards generating responses that align with our desired outcomes.\n",
        "\n",
        "## System Message\n",
        "\n",
        "To perform prompt engineering, we can start by designing a system message. A system message is the initial message provided to the AI model that sets the stage for the conversation. It frames the context and provides guidelines for the AI's behavior. While it is not part of the actual conversation, it plays a crucial role in shaping the assistant's responses.\n",
        "\n",
        "When designing a system message, consider the following questions:\n",
        "\n",
        "- What tone should the assistant use? Should it be formal, casual, or something else?\n",
        "- What background information should the assistant already know? This can include facts, previous conversations, or any other relevant context.\n",
        "- What personality and style would you like the assistant to have? Should it be friendly, professional, humorous, or something else?\n",
        "- What are your name and the assistant's name? This helps to establish a personal connection.\n",
        "\n",
        "By answering these questions, we can create a system message that provides the necessary framework for the assistant's behavior.\n",
        "\n",
        "### Essential Parts of a System Message\n",
        "\n",
        "A typical system message consists of several essential components:\n",
        "\n",
        "1. *Behavioral Guidelines*: These are recommendations or rules that define how you would like the AI to respond. For example, you might want the AI to avoid certain topics or use specific language.\n",
        "\n",
        "2. *Background Context*: This includes any information that the AI should be aware of before engaging in the conversation. It can include facts, relevant details, or previous interactions.\n",
        "\n",
        "3. *Persona*: The persona represents the personality and style of the AI system. It defines how the assistant speaks, behaves, and interacts with the user. The persona can range from being professional and formal to being more casual and friendly.\n",
        "\n",
        "To get more details about system messages and their implementation, refer to the OpenAI documentation.\n",
        "\n",
        "Here's an example of a system message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IABTw1LPpMz2"
      },
      "outputs": [],
      "source": [
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "background_context = \"\"\"\n",
        "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
        "\"\"\"\n",
        "\n",
        "persona = \"\"\"\n",
        "Act as a fun and experienced personal assistant\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "{guidelines}\n",
        "\n",
        "{background_context}\n",
        "\n",
        "{persona}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNLWdxcpe8y"
      },
      "source": [
        "\n",
        "To ensure that the guidelines are correctly set, let's print them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsca1sYpnI_",
        "outputId": "2279ec93-4ea2-4b97-afb3-590ea8aef329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Respond with at most two sentences at a time.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(guidelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to add the guidelines to our chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing\n",
            "Hello! How can I assist you with testing?\n",
            "exit\n"
          ]
        }
      ],
      "source": [
        "class Chat:\n",
        "    def __init__(self):\n",
        "        self.messages = [\n",
        "            # TODO include the system message here\n",
        "        ]\n",
        "\n",
        "    def _add_message(self, content, role):\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def _get_response(self): \n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.messages,\n",
        "            max_tokens=30\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def initiate_chat(self):\n",
        "        while True:\n",
        "            prompt = input(\"Type a prompt...\")\n",
        "            print(prompt)\n",
        "            if prompt == \"exit\":\n",
        "                break\n",
        "            self._add_message(prompt, \"user\")\n",
        "            response = self._get_response()\n",
        "            print(response)\n",
        "\n",
        "\n",
        "chat = Chat()\n",
        "chat.initiate_chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Our Model\n",
        "\n",
        "Recently, there has been a rise of domain-specific models. These are AI systems that have been further trained on a specific dataset so that they are more competent in that area. \n",
        "\n",
        "You may want your AI assistant to be an expert in a particular subject, so let's go through how we would fine-tune a model and then use it, using the OpenAI API. See what OpenAI has to say about fine-tuning here.\n",
        "\n",
        "Firstly, we need to get some data that we want to fine-tune on.\n",
        "\n",
        "THe fine-tune API expects the fine tuning text to be in a specific format called JSONL (where each line of the file is valid [JSON](https://www.json.org/json-en.html)).\n",
        "\n",
        "Check out the specification of the exact format required in the [documentation](https://platform.openai.com/docs/guides/fine-tuning/example-format).\n",
        "\n",
        "I've saved a file of the conversations between J.A.R.V.I.S and Iron Man that I got from the movie transcripts in a file called \"fine-tune-text.jsonl\". Let's check it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Good morning. It's 7 A.M. The weather in Malibu is 72 degrees with scattered clouds. The surf conditions are fair with waist to shoulder highlines, high tide will be at 10:52 a.m.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are now running on emergency backup power.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"You are not authorized to access this area.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Are you up?\"}, {\"role\": \"assistant\", \"content\": \"For you sir, always.\"}, {\"role\": \"user\", \"content\": \"I'd like to open a new project file, index as: Mark II.\"}, {\"role\": \"assistant\", \"content\": \"Shall I store this on the Stark Industries' central database?\"}, {\"role\": \"user\", \"content\": \"I don't know who to trust right now. 'Til further notice, why don't we just keep everything on my private server.\"}, {\"role\": \"assistant\", \"content\": \"Working on a secret project, are we, sir?\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"J.A.R.V.I.S., you there?\"}, {\"role\": \"assistant\", \"content\": \"At your service, sir.\"}, {\"role\": \"user\", \"content\": \"Engage heads up display.\"}, {\"role\": \"assistant\", \"content\": \"Check.\"}, {\"role\": \"user\", \"content\": \"Import all preferences from home interface.\"}, {\"role\": \"assistant\", \"content\": \"Will do, sir.\"}, {\"role\": \"user\", \"content\": \"Alright, what do you say?\"}, {\"role\": \"assistant\", \"content\": \"I have indeed been uploaded, sir. We're online and ready.\"}, {\"role\": \"user\", \"content\": \"Can we start the virtual walk-around?\"}, {\"role\": \"assistant\", \"content\": \"Importing preferences and calibrating virtual environment.\"}, {\"role\": \"user\", \"content\": \"Do a check on the control surfaces.\"}, {\"role\": \"assistant\", \"content\": \"As you wish.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Test complete. Preparing to power down and begin diagnostics.\"}, {\"role\": \"user\", \"content\": \"Uh, yeah, tell you what. Do a weather and ATC check, start listening in on ground control.\"}, {\"role\": \"assistant\", \"content\": \"Sir, there are still terabytes of calculations required before an actual flight is safe\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The altitude record for fixed wing flight is eighty-five thousand feet, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Sir, there is a potentially fatal buildup of ice occurring.\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Notes: main transducer feels sluggish at plus 40 altitude. Hull pressurization is problematic. I'm thinking icing is the probable factor.\"}, {\"role\": \"assistant\", \"content\": \"A very astute observation, sir. Perhaps if you intend to visit other planets, you should include the we should improve the exosystems.\"}, {\"role\": \"user\", \"content\": \"Connect to the sys. co. Have it reconfigure the shell metals. Use the gold titanium alloy from the seraphim tactical satellite. That should ensure a fuselage integrity while while maintaining power-to-weight ratio. Got it?\"}, {\"role\": \"assistant\", \"content\": \"Yes. Shall I render using proposed specifications?\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"The render is complete.\"}, {\"role\": \"user\", \"content\": \"A little ostentatious, don't you think?\"}, {\"role\": \"assistant\", \"content\": \"What was I thinking? You're usually so discrete.\"}, {\"role\": \"user\", \"content\": \"Tell you what, throw a little hot rod red in there.\"}, {\"role\": \"assistant\", \"content\": \"Yes, that shall help you keep a low profile.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Query complete, sir. Anton Vanko was a Soviet physicist who defected to the United States in 1963. However, he was accused of espionage and was deported in 1967. His son, Ivan, who is also a physicist, was convicted of selling Soviet-era weapons-grade plutonium to Pakistan, and served fifteen years in Kopeisk prison. No further records exist.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"Welcome home, sir. Congratulations on the opening ceremonies. They were such a success, as was your senate hearing. And may I say how refreshing it is to finally see you in a video with your clothing on, sir.\"}]}\n",
            "{\"messages\": [{\"role\": \"assistant\", \"content\": \"We are up to 80 ounces a day to counteract the symptoms, sir.\"}, {\"role\": \"user\", \"content\": \"Check Palladium levels.\"}, {\"role\": \"assistant\", \"content\": \"Blood toxicity, 24%. It appears that the continued use of the Iron Man suit is accelerating your condition. Another core has been depleted.\"}]}\n"
          ]
        }
      ],
      "source": [
        "with open(\"fine-tune-text.jsonl\") as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a file by uploading it to OpenAI. See the docs [here](https://platform.openai.com/docs/api-reference/files/create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<File file id=file-cIPczplHHjK3i5itXyQC1GB1 at 0x7fe15017bc20> JSON: {\n",
              "  \"object\": \"file\",\n",
              "  \"id\": \"file-cIPczplHHjK3i5itXyQC1GB1\",\n",
              "  \"purpose\": \"fine-tune\",\n",
              "  \"filename\": \"file\",\n",
              "  \"bytes\": 4458,\n",
              "  \"created_at\": 1697571521,\n",
              "  \"status\": \"uploaded\",\n",
              "  \"status_details\": null\n",
              "}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.File.create(\n",
        "    file=open(\"fine-tune-text.jsonl\", \"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've uploaded the file, we can start the fine-tuning job. All we need to do to do that is make a call to the API specifying which model we want to fine-tune, and which dataset we want to fine-tune it on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_41232/3856011415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tuning_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFineTuningJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"file-cIPczplHHjK3i5itXyQC1GB1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/openai/api_resources/abstract/createable_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai.FineTuningJob.create(training_file=\"file-cIPczplHHjK3i5itXyQC1GB1\", model=\"gpt-3.5-turbo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(fine_tuning_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.FineTuningJob.retrieve(\"ftjob-abc123\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can list all of our fine-tuning jobs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.FineTuningJob.list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use a fine-tuned model, we just use it's name when we specify the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"YOUR FINE TUNED MODEL ID FOUND ABOVE\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n",
        "    max_tokens=30\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "\n",
        "## Take-home Challenges\n",
        "\n",
        "1. Add a keyword argument to the `Chat` class to control whether the assistant or the user speaks first. This will allow for more flexibility in the conversation flow.\n",
        "\n",
        "2. Enhance the system message by customizing the behavioral guidelines and persona to align with your desired assistant's behavior.\n",
        "\n",
        "By completing these challenges, you will gain a deeper understanding of prompt engineering and how it can be used to shape the behavior of your AI assistant.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "Throughout this notebook, we discovered several key insights related to building AI assistants:\n",
        "\n",
        "1. Building AI assistants comes with its own set of challenges, including handling user inputs, generating coherent responses, and maintaining context.\n",
        "\n",
        "2. Making requests to AI systems involves choosing the right model and understanding its limitations and capabilities.\n",
        "\n",
        "3. The chat loop serves as the backbone of our AI assistant, allowing us to engage in dynamic conversations with users.\n",
        "\n",
        "4. Prompt engineering is a powerful technique to influence the behavior of AI models and improve the quality of responses.\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "Now that we have laid the foundation in Part I, it's time to move on to Part II: Building Tools for your AI Assistant. In Part II, we will explore how to define tools that our assistant can use to perform specific tasks. We will also dive into retrieval augmented generation, a technique that combines the benefits of both retrieval-based and generative models.\n",
        "\n",
        "In Part III, we will focus on voice interactions with our assistant. We will learn how to integrate speech recognition and synthesis to enable natural and immersive conversations.\n",
        "\n",
        "By the end of this tutorial series, you will have a comprehensive understanding of building AI assistants and will be equipped with the knowledge and skills to create your own.\n",
        "\n",
        "So let's continue our journey and move on to Part II: Building Tools for your AI Assistant!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
