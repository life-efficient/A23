{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RT3IysHWffn"
      },
      "source": [
        "# Building AI Assistants Part II: Building Tools for our AI Assistants\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%3A%20Tools/Notebook.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[Find the solutions here](https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%3A%20Tools/Solutions.ipynb)\n",
        "\n",
        "[Access Discord](https://discord.gg/SBW2zmfSMh)\n",
        "\n",
        "![](images/cyber.png)\n",
        "\n",
        "\n",
        "## Recap\n",
        "\n",
        "Last lecture, we wrote the code that allowed us to run our own ChatGPT-like system. \n",
        "It could take text input from us, and return with text responses in a back and forth conversational manner.\n",
        "Below is the code that did that.\n",
        "We're going to use this as a starting point today, so make sure you understand it and ask lots of questions if anything is unclear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "# install required python libraries\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's firstly set up the OpenAI library by importing and then providing our API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"YOUR_API_KEY\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the main body of the code that we developed in the first lecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS\n",
        "import openai\n",
        "\n",
        "\n",
        "# API KEY SETUP\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "\n",
        "# SET UP THE SYSTEM MESSAGE\n",
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "background_context = \"\"\"\n",
        "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
        "\"\"\"\n",
        "\n",
        "persona = \"\"\"\n",
        "Act as a fun and experienced personal assistant\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "{guidelines}\n",
        "\n",
        "{background_context}\n",
        "\n",
        "{persona}\n",
        "\"\"\"\n",
        "\n",
        "# MAIN FUNCTIONS\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    \"\"\"Chat with the AI system\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    \"\"\"Adds a message to the list of messages\"\"\"\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "# USE THE FUNCTIONS\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Motivation\n",
        "\n",
        "Under the hood, our AI assistant is pretty basic.\n",
        "It's just making requests to the OpenAI API.\n",
        "As powerful as that model is, there's a lot it can't do.\n",
        "\n",
        "For example:\n",
        "- Access anything on the internet, including up-to-date information.\n",
        "- Access any files that might be relevant or useful for the instructions we provide to our assistant.\n",
        "- Plug in to other applications that we use, like email, search, and so on.\n",
        "\n",
        "If we really want our AI assistant to be useful, it needs to be able to do these things!\n",
        "\n",
        "So today, we are going to build some tools that unlock these capabilities for our AI assistant.\n",
        "\n",
        "## In this lecture:\n",
        "- Various examples of different kinds of tools that you might want to build for your AI system:\n",
        "    - Tools that access your internal filesystem.\n",
        "    - Tools that use external APIs.\n",
        "    - Tools that require lots of surrounding Python logic.\n",
        "- Retrieval Augmented Generation (RAG): Grounding LLM responses in ground truth data.\n",
        "\n",
        "## Exploring how to get started building tools\n",
        "\n",
        "If you didn't have any idea of how to do this, the first place you would look is the [documentation](https://platform.openai.com/docs/api-reference).\n",
        "\n",
        "### Challenge: Take 5 minutes to explore and see if you can figure out which part of the [docs](https://platform.openai.com/docs/api-reference) might be useful\n",
        "\n",
        "Here are some hints:\n",
        "1. We want our system to be able to choose to use a tool when we ask it for a chat response.\n",
        "2. When we define tools that our assistant can use, we're probably going to define them as a function.\n",
        "\n",
        "### In case you didn't figure it out for yourself, here's how it works\n",
        "\n",
        "In short:\n",
        "1. You define a function\n",
        "2. You provide a list of functions that the API can choose to call when it creates a chat completion (instead of responding with a text response)\n",
        "3. When appropriate, the model responds with the name of the function and the parameters that it believes should be passed to the function. \n",
        "\n",
        "This is the [important part](https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions) of the documentation that you should have found, and should take time to understand now.\n",
        "\n",
        "A few things to note:\n",
        "- It is important to note that the model does not actually call the function, instead it provides a response including the name of the function that it wants to call along with the relevant parameters it wants to use. It's then up to you to implement the code to call that function with those parameters.\n",
        "- The large language model (LLM) uses the description of the function provided in it's definition to determine what a function does, and hence when it should be used.\n",
        "\n",
        "## Now it's time to build our first tool for our assistant\n",
        "\n",
        "Here's our code that is currently processing the responses of our AI assistant. \n",
        "\n",
        "> #### When we enable the system to call functions, we're going to need to change this slightly as we will see in a second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "dummy_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Howdy\"},\n",
        "]\n",
        "\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content # NOTE: can you anticipate what's going to go wrong with this line when our model chooses to call a function instead of returning text?\n",
        "    return content\n",
        "\n",
        "normal_response = get_response(dummy_messages)\n",
        "print(normal_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge: Use the docs to create a dummy tool that our assistant can use that prints the current time\n",
        "\n",
        "Note: if your dummy function performs a task that can be done by the language model without the need for an additional tool (e.g. printing hello) then it's unlikely that the LLM will choose to use the tool. Instead, it will just do it without."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time # TODO import the time function from the time module\n",
        "\n",
        "def get_current_time(): # TODO define a function to print out the current time\n",
        "    print(time())\n",
        "\n",
        "tools = [{ # TODO define a list of tools that our AI system can use in the format requested by the OpenAI API\n",
        "    \"name\": \"get_current_time\",\n",
        "    \"description\": \"Prints out the current time\",\n",
        "    \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's give this toolkit to our AI system so that it can choose to call the function if it needs.\n",
        "\n",
        "> (Again, expect a minor error that we're going to have to correct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "Hi there! How can I assist you today?\n",
            "Function call response\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        functions=tools, # TODO pass the list of tools to the AI system when we make a request\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function call response is `None`!\n",
        "\n",
        "### Challenge: Answer \"Why?\" by printing out parts of your code above to find where it happens, then solve the bug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "Hi there! How can I assist you today?\n",
            "Function call response\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# TODO fix the bug by changing part of the code below\n",
        "\n",
        "\n",
        "# TODO print things inside this function out and identify where the bug occurs. Where does the `None` come from?\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        functions=tools,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(\n",
        "    messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The issue is on this line \n",
        "\n",
        "```\n",
        "    content = response.choices[0].message.content\n",
        "```\n",
        "\n",
        "The problem is that the `content` attribute of the message is `None` when the AI system suggests calling a function.\n",
        "\n",
        "So let's return the message rather than assuming that the message will always have a content attribute.\n",
        "Note how the LLM responses look different when a function is suggested vs not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"Hi there! How can I assist you today?\"\n",
            "}\n",
            "Function call response\n",
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": null,\n",
            "  \"function_call\": {\n",
            "    \"name\": \"get_current_time\",\n",
            "    \"arguments\": \"{}\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# TODO fix the bug by changing part of the code below\n",
        "\n",
        "\n",
        "# TODO print things inside this function out and identify where the bug occurs. Where does the `None` come from?\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        functions=tools,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message # TODO return message, not message.content\n",
        "    return content\n",
        "\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(\n",
        "    messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just quickly, the AI system only returns the name of the function that should be used. We need the actual function, not just its name. One way we can do this is to create a dictionary where the keys are the name of the different tools and the values are the functions themselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_names_to_functions = {\n",
        "    \"get_current_time\": get_current_time\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to adapt our code so that it can deal with both cases, either calling the suggested function or printing out the text response, depending on which the LLM decides to provide.\n",
        "\n",
        "To understand how to do that, let's:\n",
        "1. Define a function which determines whether a function should be called or not.\n",
        "2. Define a function which calls the appropriate function with the appropriate parameters. Once the AI system has generated a response that suggests a function call is necessary, it's up to us to implement the code that calls the function.\n",
        "3. Define a function, which overall handles the assistant's response, either calling the suggested function or printing the text response, and then returns the updated list of messages in the conversation so far.\n",
        "\n",
        "> Note: Messages that suggest function calls should be included in the message history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function call suggested\n",
            "Calling get_current_time with parameters {}\n",
            "1698720064.188245\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': \"What's the time right now?\"},\n",
              " <OpenAIObject at 0x7f83961654a0> JSON: {\n",
              "   \"role\": \"assistant\",\n",
              "   \"content\": null,\n",
              "   \"function_call\": {\n",
              "     \"name\": \"get_current_time\",\n",
              "     \"arguments\": \"{}\"\n",
              "   }\n",
              " },\n",
              " <OpenAIObject at 0x7f83961654a0> JSON: {\n",
              "   \"role\": \"assistant\",\n",
              "   \"content\": null,\n",
              "   \"function_call\": {\n",
              "     \"name\": \"get_current_time\",\n",
              "     \"arguments\": \"{}\"\n",
              "   }\n",
              " }]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def should_function_be_called(assistant_response):\n",
        "    \"\"\"Returns true if the assistant response contains a function call, otherwise returns false\"\"\"\n",
        "    if \"function_call\" in assistant_response.keys(): # TODO check if the assistant response contains a function call\n",
        "        print(\"Function call suggested\")\n",
        "        print(f\"Calling {assistant_response['function_call']['name']} with parameters {assistant_response['function_call']['arguments']}\")\n",
        "        return True\n",
        "    else: # TODO otherwise\n",
        "        print(\"No function call suggested\")\n",
        "        return False\n",
        "\n",
        "def call_suggested_function(assistant_response):\n",
        "    \"\"\"Calls the function suggested by the assistant with the parameters (arguments) provided\"\"\"\n",
        "    function_name = assistant_response[\"function_call\"][\"name\"] # TODO get the function name from the assistant response\n",
        "    function_parameters = assistant_response[\"function_call\"][\"arguments\"] # TODO get the function parameters from the assistant response\n",
        "    function_parameters = json.loads(function_parameters) # TODO the AI system actually returns a valid dictionary of parameters as a string, so convert the function parameters from a string to a dictionary (hint: use json.loads)\n",
        "    function = tool_names_to_functions[function_name] # TODO get the function from the list of tools\n",
        "    function(**function_parameters) # TODO call the function with the parameters (use dictionary unpacking you're a pro, otherwise a for loop)\n",
        "\n",
        "def handle_assistant_response(assistant_response, messages):\n",
        "    \"\"\"Takes in the assistant response and the messages so far, handles them, then returns the updated list of messages\"\"\"\n",
        "    if should_function_be_called(assistant_response): # TODO check if a function call was suggested\n",
        "        call_suggested_function(assistant_response) # TODO call the function to , which will call that function if so\n",
        "    else: # TODO otherwise\n",
        "        print(\"Assistant:\", assistant_response.content) # TODO print the assistant response\n",
        "    messages.append(assistant_response) # TODO add the assistant response to the list of messages (even if it's a function call)\n",
        "    return messages # TODO return the updated list of messages\n",
        "\n",
        "handle_assistant_response(function_call_response, messages_that_should_trigger_function_call) # handle the response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we've got our code to call a function if suggested, let's incorporate that into our chat loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "No function call suggested\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: what time is it?\n",
            "Function call suggested\n",
            "Calling get_current_time with parameters {}\n",
            "1698721037.114526\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS\n",
        "import openai\n",
        "\n",
        "\n",
        "# API KEY SETUP\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "\n",
        "# SET UP THE SYSTEM MESSAGE\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "# MAIN FUNCTIONS\n",
        "\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        functions=tools\n",
        "    )\n",
        "    content = response.choices[0].message\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    \"\"\"Chat with the AI system\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = handle_assistant_response(response, messages) # TODO use our newly defined function to handle the response\n",
        "        # messages = add_message(messages, response, \"assistant\") # TODO remove this line because we're now adding the assistant response inside the handle_assistant_response function\n",
        "        # print(\"Assistant:\", response) # TODO remove this line because we're now printing the assistant response inside the handle_assistant_response function\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    \"\"\"Adds a message to the list of messages\"\"\"\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "# USE THE FUNCTIONS\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now you've successfully set up a system that can call any suggested function... the world is yours!\n",
        "\n",
        "You can build any capability that you wish your AI system had now. All you need to do now is to:\n",
        "1. define more tools\n",
        "2. add them to the list of tools\n",
        "3. and then provide that list of tools to your AI assistant.\n",
        "\n",
        "### Bonus challenges: \n",
        "- Create a tool that prints out the current time and date.\n",
        "- Create a tool that prints out the current CPU utilisation.\n",
        "- Create a tool that can save the chat history so far in a file.\n",
        "- Create a tool that prints out how long the current program has been running for (use a class if you're a pro).\n",
        "- Create a tool that can write code and save it in a new file for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tools that use external APIs\n",
        "\n",
        "### Challenge: Use the OpenAI API to build a tool that allows your AI assistant to generate images and save them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: what's good?\n",
            "No function call suggested\n",
            "Assistant: As your personal assistant, I am ready to assist you with any tasks or questions you may have. How can I help you today?\n",
            "User: what time is it?\n",
            "Function call suggested\n",
            "Calling get_current_time with parameters {}\n",
            "1698721058.004361\n",
            "User: can you generate me an image of a cool synthwave dog?\n",
            "Function call suggested\n",
            "Calling generate_image with parameters {\n",
            "  \"prompt\": \"cool synthwave dog\",\n",
            "  \"image_file_name\": \"synthwave_dog.png\"\n",
            "}\n",
            "Check out the generated image here: https://oaidalleapiprodscus.blob.core.windows.net/private/org-s2O95FZfCNih64Qs50xGE3u0/user-lmW9c28kA8I68Rc3pb90okZc/img-HaYPwjENEk97miAH5T6rGfoD.png?st=2023-10-31T01%3A58%3A01Z&se=2023-10-31T03%3A58%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-10-31T00%3A38%3A42Z&ske=2023-11-01T00%3A38%3A42Z&sks=b&skv=2021-08-06&sig=RSF9XHnAUFnWIgIajCrccdiJKXldFCnHObi0mE5Z1Lg%3D\n",
            "Saved as synthwave_dog.png\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def generate_image(prompt, image_file_name):\n",
        "    \"\"\"Takes in a descrtiption of an image and a filename, then generates that image and saves it with that filename\"\"\"\n",
        "    response = openai.Image.create( # TODO use the openai Image API to generate an image\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "    img_url = response.data[0].url\n",
        "    print(\"Check out the generated image here:\", img_url)\n",
        "    image_bytes = requests.get(img_url).content\n",
        "    with open(image_file_name, \"wb\") as f: # TODO save the image to the file name provided\n",
        "        f.write(image_bytes)\n",
        "    print(f\"Saved as {image_file_name}\")\n",
        "\n",
        "# TODO append the definition of the generate_image function to the list of tools\n",
        "tools.append({\n",
        "    \"name\": \"generate_image\",\n",
        "    \"description\": \"Generates an image from a description of that image, like you might see a description below each image in a gallery\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"prompt\": {\"type\": \"string\"},\n",
        "            \"image_file_name\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"prompt\", \"image_file_name\"]\n",
        "    }\n",
        "})\n",
        "\n",
        "tool_names_to_functions[\"generate_image\"] = generate_image # TODO add the generate_image function to the dictionary of tool names to functions\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Bonus Challenges: \n",
        "- Show the generated image in the notebook.\n",
        "- Improve the prompt by applying image prompting techniques from the [DALL-E prompt book](https://dallery.gallery/the-dalle-2-prompt-book/)\n",
        "- Build a tool to get the current weather in a location. Want your assistant to be able to tell you what the weather is like where you are or where you are going?\n",
        "- Build a tool that can send emails for you.\n",
        "- Build a tool that can update entries in a CRM for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retreival Augmented Generation (RAG)\n",
        "\n",
        "One of the outstanding issues with LLMs is that they can be prone to _hallucination_ - where they confidently articulate false information.\n",
        "\n",
        "During the distillation of knowledge found in the training dataset, much of the detail can be lost, and when responding to future requests the system can fill in the gaps with false information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO helper function to break down a PDF document into paragraphs (use the pdfplumber library) and store in a hierarchical folder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Index of Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO iterate recursively through a folder structure \n",
        "# TODO index each piece of content using the embeddings API\n",
        "# TODO store the embeddings in a numpy matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO define a function to turn a query into an embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define a function that finds the nearest embedding in the index to the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO define a function to find the k nearest neighbor index embeddings to a query embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO pass the content of the embeddding into the context of the prompt before asking for a response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "\n",
        "Questions\n",
        "- What are some other key modules that you'd like your assistant to be able to do?\n",
        "\n",
        "## Further Challenges\n",
        "\n",
        "- Code up a new capability for your AI assistant using what you've learnt above\n",
        "- Implement RAG for internet searches\n",
        "\n",
        "\n",
        "## Next steps\n",
        "Make sure you're subscribed to the community to receive the following lecture materials.\n",
        "\n",
        "Make sure you're in the online [Discord community](https://discord.gg/SBW2zmfSMh)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
