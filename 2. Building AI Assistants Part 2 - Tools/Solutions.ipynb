{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RT3IysHWffn"
      },
      "source": [
        "# Building AI Assistants Part II: Building Tools for our AI Assistants\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%3A%20Tools/Notebook.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[Find the solutions here](https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%3A%20Tools/Solutions.ipynb)\n",
        "\n",
        "[Access Discord](https://discord.gg/SBW2zmfSMh)\n",
        "\n",
        "![](images/cyber.png)\n",
        "\n",
        "\n",
        "## Recap\n",
        "\n",
        "Last lecture, we wrote the code that allowed us to run our own ChatGPT-like system. \n",
        "It could take text input from us, and return with text responses in a back and forth conversational manner.\n",
        "Below is the code that did that.\n",
        "We're going to use this as a starting point today, so make sure you understand it and ask lots of questions if anything is unclear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (2.4.2)\n",
            "Requirement already satisfied: tqdm>4 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: certifi in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "# install required python libraries\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's firstly set up the OpenAI library by importing and then providing our API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "openai = OpenAI(api_key=\"YOUR_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the main body of the code that we developed in the first lecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS\n",
        "from openai import OpenAI\n",
        "# openai = OpenAI(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "\n",
        "# API KEY SETUP\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "\n",
        "# SET UP THE SYSTEM MESSAGE\n",
        "guidelines = \"\"\"\n",
        "Respond with at most two sentences at a time.\n",
        "\"\"\"\n",
        "\n",
        "background_context = \"\"\"\n",
        "You are a personal assistant for [YOUR NAME], who has a background in [YOUR BACKGROUND].\n",
        "\"\"\"\n",
        "\n",
        "persona = \"\"\"\n",
        "Act as a fun and experienced personal assistant\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "{guidelines}\n",
        "\n",
        "{background_context}\n",
        "\n",
        "{persona}\n",
        "\"\"\"\n",
        "\n",
        "# MAIN FUNCTIONS\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    \"\"\"Chat with the AI system\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = add_message(messages, response, \"assistant\")\n",
        "        print(\"Assistant:\", response)\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    \"\"\"Adds a message to the list of messages\"\"\"\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "# USE THE FUNCTIONS\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Motivation\n",
        "\n",
        "Under the hood, our AI assistant is pretty basic.\n",
        "It's just making requests to the OpenAI API.\n",
        "As powerful as that model is, there's a lot it can't do.\n",
        "\n",
        "For example:\n",
        "- Access anything on the internet, including up-to-date information.\n",
        "- Access any files that might be relevant or useful for the instructions we provide to our assistant.\n",
        "- Plug in to other applications that we use, like email, search, and so on.\n",
        "\n",
        "If we really want our AI assistant to be useful, it needs to be able to do these things!\n",
        "\n",
        "So today, we are going to build some tools that unlock these capabilities for our AI assistant.\n",
        "\n",
        "## In this lecture:\n",
        "- Various examples of different kinds of tools that you might want to build for your AI system:\n",
        "    - Tools that access your internal filesystem.\n",
        "    - Tools that use external APIs.\n",
        "    - Tools that require lots of surrounding Python logic.\n",
        "- Retrieval Augmented Generation (RAG): Grounding LLM responses in ground truth data.\n",
        "\n",
        "## Exploring how to get started building tools\n",
        "\n",
        "If you didn't have any idea of how to do this, the first place you would look is the [documentation](https://platform.openai.com/docs/api-reference).\n",
        "\n",
        "### Challenge: Take 5 minutes to explore and see if you can figure out which part of the [docs](https://platform.openai.com/docs/api-reference) might be useful\n",
        "\n",
        "Here are some hints:\n",
        "1. We want our system to be able to choose to use a tool when we ask it for a chat response.\n",
        "2. When we define tools that our assistant can use, we're probably going to define them as a function.\n",
        "\n",
        "### In case you didn't figure it out for yourself, here's how it works\n",
        "\n",
        "In short:\n",
        "1. You define a function\n",
        "2. You provide a list of functions that the API can choose to call when it creates a chat completion (instead of responding with a text response)\n",
        "3. When appropriate, the model responds with the name of the function and the parameters that it believes should be passed to the function. \n",
        "\n",
        "This is the [important part](https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions) of the documentation that you should have found, and should take time to understand now.\n",
        "\n",
        "A few things to note:\n",
        "- It is important to note that the model does not actually call the function, instead it provides a response including the name of the function that it wants to call along with the relevant parameters it wants to use. It's then up to you to implement the code to call that function with those parameters.\n",
        "- The large language model (LLM) uses the description of the function provided in it's definition to determine what a function does, and hence when it should be used.\n",
        "\n",
        "## Now it's time to build our first tool for our assistant\n",
        "\n",
        "Here's our code that is currently processing the responses of our AI assistant. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "dummy_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Howdy\"},\n",
        "]\n",
        "\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content # NOTE: can you anticipate what's going to go wrong with this line when our model chooses to call a function instead of returning text?\n",
        "    return content\n",
        "\n",
        "normal_response = get_response(dummy_messages)\n",
        "print(normal_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge: Use the docs to create a dummy tool that our assistant can use that prints the current time\n",
        "\n",
        "Note: if your dummy function performs a task that can be done by the language model without the need for an additional tool (e.g. printing hello) then it's unlikely that the LLM will choose to use the tool. Instead, it will just do it without.\n",
        "\n",
        "Make sure to lean heavily on the [documentation](https://platform.openai.com/docs/api-reference/chat/create)... it is very unlikely you're going to be able to guess the exact format required to provide your LLM with tools.\n",
        "\n",
        "Note: As we will see later, it's important that the function return a value so that the LLM can use the information returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time # TODO import the time function from the time module\n",
        "\n",
        "# TODO #1 define a function to print out the current time\n",
        "def get_current_time(): \n",
        "    t = time()\n",
        "    print(\"Running function to print time:\", t)\n",
        "    return t\n",
        "\n",
        "# TODO define a list of tools that our AI system can use in the format requested by the OpenAI API\n",
        "my_tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_time\",\n",
        "            \"description\": \"Prints out the current time\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\", \n",
        "                \"properties\": {}\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's give this toolkit to our AI system so that it can choose to call the function if it needs.\n",
        "\n",
        "> Expect a minor error that we're going to have to correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "Hi there! How can I assist you today?\n",
            "Function call response\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        tools=my_tools, # TODO pass the list of tools to the AI system when we make a request\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function call response is `None`! Why?\n",
        "\n",
        "### Challenge: Find out why the function call response is `None` by printing out parts of your code above to find where it happens, then solve the bug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "Hi there! How can I assist you today?\n",
            "Function call response\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# TODO fix the bug by changing part of the code below\n",
        "\n",
        "\n",
        "# TODO print things inside this function out and identify where the bug occurs. Where does the `None` come from?\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        tools=my_tools,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    return content\n",
        "\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(\n",
        "    messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The issue is on this line \n",
        "\n",
        "```\n",
        "    content = response.choices[0].message.content\n",
        "```\n",
        "\n",
        "The problem is that the `content` attribute of the message is `None` when the AI system suggests calling a function.\n",
        "\n",
        "So let's return the message rather than assuming that the message will always have a content attribute.\n",
        "Note how the LLM responses look different when a function is suggested vs not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal response\n",
            "ChatCompletionMessage(content='Hi there! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)\n",
            "Function call response\n",
            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_nP6p1QrGMx5YZPdBqul7Csf4', function=Function(arguments='{}', name='get_current_time'), type='function')])\n"
          ]
        }
      ],
      "source": [
        "# TODO fix the bug by changing part of the code below\n",
        "\n",
        "\n",
        "# TODO print things inside this function out and identify where the bug occurs. Where does the `None` come from?\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        tools=my_tools,\n",
        "        max_tokens=30\n",
        "    )\n",
        "    content = response.choices[0].message # TODO return message, not message.content\n",
        "    return content\n",
        "\n",
        "\n",
        " # make a normal request and look at the response\n",
        "normal_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "]\n",
        "normal_response = get_response(normal_messages)\n",
        "print('Normal response')\n",
        "print(normal_response)\n",
        "\n",
        "# make a request that uses the tool and look at the response\n",
        "messages_that_should_trigger_function_call = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the time right now?\"}\n",
        "]\n",
        "function_call_response = get_response(\n",
        "    messages_that_should_trigger_function_call)\n",
        "print('Function call response')\n",
        "print(function_call_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just quickly, the AI system only returns the name of the function that should be used. We need the actual function, not just its name. One way we can do this is to create a dictionary where the keys are the name of the different tools and the values are the functions themselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running function to print time: 1699359565.625082\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1699359565.625082"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_names_to_functions = {\n",
        "    \"get_current_time\": get_current_time\n",
        "}\n",
        "\n",
        "# EXAMPLE USAGE\n",
        "name_of_function = \"get_current_time\" # this is just a string - the name of the function. This is what the LLM returns\n",
        "# name_of_function() # calling this would give an error - you can't call strings!\n",
        "actual_function_which_can_be_called = tool_names_to_functions[name_of_function] # this gets the actual function from the dictionary\n",
        "actual_function_which_can_be_called() # this is a function which can be called by putting parentheses () after it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to adapt our code so that it can deal with both cases, either calling the suggested function or printing out the text response, depending on which the LLM decides to provide.\n",
        "\n",
        "To understand how to do that, let's:\n",
        "1. Define a function which determines whether a function should be called or not.\n",
        "2. Define a function which calls the appropriate function with the appropriate parameters. Once the AI system has generated a response that suggests a function call is necessary, it's up to us to implement the code that calls the function.\n",
        "3. Define a function, which overall handles the assistant's response, either calling the suggested function or printing the text response, and then returns the updated list of messages in the conversation so far.\n",
        "\n",
        "> Note: Messages that suggest function calls should be included in the message history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function call suggested\n",
            "Calling get_current_time with parameters {}\n",
            "calling function: ChatCompletionMessageToolCall(id='call_nP6p1QrGMx5YZPdBqul7Csf4', function=Function(arguments='{}', name='get_current_time'), type='function')\n",
            "Running function to print time: 1699359929.143368\n",
            "No function calls suggested\n",
            "Assistant: The current time is 1699359929.143368.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': \"What's the time right now?\"},\n",
              " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_nP6p1QrGMx5YZPdBqul7Csf4', function=Function(arguments='{}', name='get_current_time'), type='function')]),\n",
              " {'role': 'tool',\n",
              "  'content': '1699359929.143368',\n",
              "  'tool_call_id': 'call_nP6p1QrGMx5YZPdBqul7Csf4'},\n",
              " ChatCompletionMessage(content='The current time is 1699359929.143368.', role='assistant', function_call=None, tool_calls=None)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from pprint import pprint\n",
        "import json\n",
        "\n",
        "def should_function_be_called(assistant_response):\n",
        "    \"\"\"Returns true if the assistant response contains a function call, otherwise returns false\"\"\"\n",
        "    if assistant_response.tool_calls is not None: # TODO check if the assistant response contains a function call\n",
        "        for tool_call in assistant_response.tool_calls: # TODO the LLM can output multiple tool calls in one response, so loop through them\n",
        "            print(\"Function call suggested\")\n",
        "            print(f\"Calling {tool_call.function.name} with parameters {tool_call.function.arguments}\")\n",
        "        return True\n",
        "    else: # TODO otherwise\n",
        "        print(\"No function calls suggested\")\n",
        "        return False\n",
        "\n",
        "def call_suggested_functions(assistant_response):\n",
        "    \"\"\"Calls the function suggested by the assistant with the parameters (arguments) provided\"\"\"\n",
        "    list_of_tool_response_messages = [] # TODO create an empty list of tool response messages\n",
        "    for tool_call in assistant_response.tool_calls: # TODO loop through the tool calls in the assistant response\n",
        "        print(\"calling function:\", tool_call)\n",
        "        function_name = tool_call.function.name # TODO get the function name from the assistant response\n",
        "        function_parameters = tool_call.function.arguments # TODO get the function parameters from the assistant response\n",
        "        function_parameters = json.loads(function_parameters) # TODO the AI system actually returns a valid dictionary of parameters as a string, so convert the function parameters from a string to a dictionary (hint: use json.loads)\n",
        "        function = tool_names_to_functions[function_name] # TODO get the function from the list of tools\n",
        "        return_value = function(**function_parameters) # TODO call the function with the parameters (use dictionary unpacking you're a pro, otherwise a for loop)\n",
        "        tool_response_message = { # TODO create a tool response message in the format requested by the OpenAI API\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": json.dumps(return_value),\n",
        "            \"tool_call_id\": tool_call.id\n",
        "        }\n",
        "        list_of_tool_response_messages.append(tool_response_message) # TODO add the tool response message to the list of tool response messages\n",
        "    return list_of_tool_response_messages # TODO return the list of tool response messages\n",
        "\n",
        "def handle_assistant_response(assistant_response, messages):\n",
        "    \"\"\"Takes in the assistant response and the messages so far, handles them, then returns the updated list of messages\"\"\"\n",
        "    messages.append(assistant_response) # TODO add the assistant response to the list of messages (even if it's a function call)\n",
        "    if should_function_be_called(assistant_response): # TODO check if a function call was suggested\n",
        "        tool_response_messages = call_suggested_functions(assistant_response) # TODO call the function\n",
        "        messages.extend(tool_response_messages) # TODO extend the list of messages with the tool response messages\n",
        "        messages = handle_assistant_response(get_response(messages), messages) # TODO (advanced) allow the assistant to comment on the response provided by the tool\n",
        "    else: # TODO otherwise\n",
        "        print(\"Assistant:\", assistant_response.content) # TODO print the assistant response\n",
        "    return messages # TODO return the updated list of messages\n",
        "\n",
        "messages_that_should_trigger_function_call = [{'role': 'user', 'content': \"What's the time right now?\"}]\n",
        "handle_assistant_response(function_call_response, messages_that_should_trigger_function_call) # handle the response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we've got our code to call a function if suggested, let's incorporate that into our chat loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: helo\n",
            "No function calls suggested\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS\n",
        "import openai\n",
        "\n",
        "\n",
        "# API KEY SETUP\n",
        "# openai.api_key = \"YOUR API KEY HERE\" # commented out so you don't overwrite the correct api key you set above\n",
        "\n",
        "\n",
        "# SET UP THE SYSTEM MESSAGE\n",
        "# your system message probably has a lot of text in that you would need to copy over, so I haven't included it down here\n",
        "\n",
        "# MAIN FUNCTIONS\n",
        "\n",
        "def get_response(messages, fine_tuned_model_id=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Gets a response from an AI system given a list of messages so far\"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        tools=my_tools\n",
        "    )\n",
        "    content = response.choices[0].message\n",
        "    return content\n",
        "\n",
        "\n",
        "def chat():\n",
        "    \"\"\"Chat with the AI system\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message}\n",
        "    ]\n",
        "    while True:\n",
        "        prompt = input(\"Type a prompt...\")\n",
        "        print(\"User:\", prompt)\n",
        "        if prompt == \"exit\":\n",
        "            break\n",
        "        messages = add_message(messages, prompt, \"user\")\n",
        "        response = get_response(messages)\n",
        "        messages = handle_assistant_response(response, messages) # TODO use our newly defined function to handle the response\n",
        "        # messages = add_message(messages, response, \"assistant\") # TODO remove this line because we're now adding the assistant response inside the handle_assistant_response function\n",
        "        # print(\"Assistant:\", response) # TODO remove this line because we're now printing the assistant response inside the handle_assistant_response function\n",
        "\n",
        "\n",
        "def add_message(messages, content, role):\n",
        "    \"\"\"Adds a message to the list of messages\"\"\"\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "# USE THE FUNCTIONS\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now you've successfully set up a system that can call any suggested function... the world is yours!\n",
        "\n",
        "You can build any capability that you wish your AI system had now. All you need to do now is to:\n",
        "1. define more tools\n",
        "2. add them to the list of tools\n",
        "3. and then provide that list of tools to your AI assistant.\n",
        "\n",
        "### Bonus challenges: \n",
        "- Create a tool that prints out the current time and date.\n",
        "- Create a tool that prints out the current CPU utilisation.\n",
        "- Create a tool that can save the chat history so far in a file.\n",
        "- Create a tool that prints out how long the current program has been running for (use a class if you're a pro).\n",
        "- Create a tool that can write code and save it in a new file for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tools that use external APIs\n",
        "\n",
        "### Challenge: Use the OpenAI API to build a tool that allows your AI assistant to generate images and save them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "No function calls suggested\n",
            "Assistant: Hello! How can I assist you today?\n",
            "User: generate an image of an animal of your choice. include the name of the animal in your prompt as well as the scene that you would like that animal to be found in.\n",
            "Function call suggested\n",
            "Calling generate_image with parameters {\n",
            "  \"prompt\": \"A playful dolphin swimming in the crystal clear waters of a tropical island.\",\n",
            "  \"image_file_name\": \"dolphin.jpg\"\n",
            "}\n",
            "calling function: ChatCompletionMessageToolCall(id='call_dow7wLLUg2kcl2sxR8FzbPxp', function=Function(arguments='{\\n  \"prompt\": \"A playful dolphin swimming in the crystal clear waters of a tropical island.\",\\n  \"image_file_name\": \"dolphin.jpg\"\\n}', name='generate_image'), type='function')\n",
            "Check out the generated image here: https://oaidalleapiprodscus.blob.core.windows.net/private/org-DCX94PX8uNItZF8Tb6T5cdX2/user-poQxBDwazoeArT4IA9xgQpTj/img-ql0a3zV0wLIay1Zx5YNX6QRQ.png?st=2023-11-07T11%3A35%3A33Z&se=2023-11-07T13%3A35%3A33Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-11-07T11%3A26%3A04Z&ske=2023-11-08T11%3A26%3A04Z&sks=b&skv=2021-08-06&sig=SuJ08rq5bcgMYmPl7ZsQPOaDBKZd94jGLn2lD/X3Cj8%3D\n",
            "Saved as dolphin.jpg\n",
            "No function calls suggested\n",
            "Assistant: I apologize, but I'm unable to generate images. However, you can easily find images like the one you described by searching online.\n",
            "User: what time is it?\n",
            "Function call suggested\n",
            "Calling get_current_time with parameters {}\n",
            "calling function: ChatCompletionMessageToolCall(id='call_mxxkNtBkLXn01uRyFRW2X8Kv', function=Function(arguments='{}', name='get_current_time'), type='function')\n",
            "Running function to print time: 1699360559.92877\n",
            "No function calls suggested\n",
            "Assistant: The current time is 12:35 PM.\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def generate_image(prompt, image_file_name):\n",
        "    \"\"\"Takes in a descrtiption of an image and a filename, then generates that image and saves it with that filename\"\"\"\n",
        "    response = openai.images.generate( # TODO use the openai Image API to generate an image\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "    img_url = response.data[0].url\n",
        "    print(\"Check out the generated image here:\", img_url)\n",
        "    image_bytes = requests.get(img_url).content\n",
        "    with open(image_file_name, \"wb\") as f: # TODO save the image to the file name provided\n",
        "        f.write(image_bytes)\n",
        "    print(f\"Saved as {image_file_name}\")\n",
        "\n",
        "# TODO append the definition of the generate_image function to the list of tools\n",
        "my_tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_time\",\n",
        "            \"description\": \"Prints out the current time\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\", \n",
        "                \"properties\": {}\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"generate_image\",\n",
        "            \"description\": \"Generates an image from a description of that image, like you might see a description below each image in a gallery\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"prompt\": {\"type\": \"string\"},\n",
        "                    \"image_file_name\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"prompt\", \"image_file_name\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "tool_names_to_functions[\"generate_image\"] = generate_image # TODO add the generate_image function to the dictionary of tool names to functions\n",
        "\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Bonus Challenges: \n",
        "- Show the generated image in the notebook.\n",
        "- Improve the prompt by applying image prompting techniques from the [DALL-E prompt book](https://dallery.gallery/the-dalle-2-prompt-book/)\n",
        "- Build a tool to get the current weather in a location. Want your assistant to be able to tell you what the weather is like where you are or where you are going?\n",
        "- Build a tool that can send emails for you.\n",
        "- Build a tool that can update entries in a CRM for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQD-ip7prQi"
      },
      "source": [
        "\n",
        "Questions\n",
        "- What are some other key modules that you'd like your assistant to be able to do?\n",
        "\n",
        "## Further Challenges\n",
        "\n",
        "- Code up a new capability for your AI assistant using what you've learnt above\n",
        "- Implement RAG for internet searches\n",
        "\n",
        "\n",
        "## Next steps\n",
        "Make sure you're subscribed to the community to receive the following lecture materials.\n",
        "\n",
        "Make sure you're in the online [Discord community](https://discord.gg/SBW2zmfSMh)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
