{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tame an LLM using Retrieval Augmented Generation (RAG)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/RAG/Notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "[Find the solutions here](https://colab.research.google.com/github/life-efficient/A23/blob/main/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/RAG/Solutions.ipynb)\n",
    "\n",
    "[Access Discord](https://discord.gg/SBW2zmfSMh)\n",
    "\n",
    "In this lecture:\n",
    "- The problem of LLM hallucination\n",
    "- Guide prompts that include reference material\n",
    "- Retrieval Augmented Generation (RAG)\n",
    "    - Embedding\n",
    "    - Vector stores\n",
    "    - Closest neighbour search\n",
    "\n",
    "### How do LLMs really work?\n",
    "- This is a useful thought experiment. ([Sidenote: This is where it all started](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "- Turns out they're fairly similar to us in some regards (long-term & short-term 'memory', ability to '[pay attention](https://arxiv.org/pdf/2307.03172.pdf)').\n",
    "- Where possible, put LLMs in position to use short-term memory and help them pay attention.\n",
    "- LLMs are great at [pattern matching](https://arxiv.org/abs/2005.14165) and following syntactic rules.\n",
    "- There are cases where LLMs provide a solution to a problem, but [they may be suboptimal](https://aclanthology.org/2023.findings-acl.426.pdf).\n",
    "\n",
    "### Retrieval Augmented Generation (RAG) - An Antidote to Hallucination\n",
    "- One such way of limiting the use of LLMs to what they are best at.\n",
    "\n",
    "- Uses [in-context learning](https://arxiv.org/abs/2301.00234) to give the LLM a usable short-term memory.\n",
    "\n",
    "Let's use this to ask questions about some lecture notes\n",
    "### Reading my PDF in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "reader = PdfReader(\"Lecture Notes.pdf\") # read the pdf file\n",
    "\n",
    "# Read each page and store them as a string\n",
    "lecture_notes  = ''.join([page.extract_text() for page in reader.pages]) # create one big string of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai from openai import OpenAI
client = OpenAI()\n",
    "import os\n",
    "\n",
    "# Load our OpenAI API key\n",
    "openai = OpenAI(api_key=os.getenv(\"OPEN_AI_API_KEY\")) # TODO set your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Generation - RAG's little brother\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query (query, context):\n",
    "\n",
    "    # Tell the LLM to only use the data we give it\n",
    "    guide_prompt = # TODO create a guide prompt that asks the LLM to answer the query using the context\n",
    "    messages = # TODO create a list of messages that ends with the quide prompt\n",
    "\n",
    "    response = # TODO create a completion using the guide prompt\n",
    "\n",
    "    response_content = # TODO get the response content from the response\n",
    "\n",
    "    return(response_content)\n",
    "\n",
    "# TODO ask the LLM to answer the query using the lecture notes as context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if the lecture notes were twice as long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_query(query=\"What is an objective function?\", context=lecture_notes*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we hit an issue: The context provided (the entire document of notes) is too large to be processed by the LLM.\n",
    "\n",
    "One solution might be to use an LLM with a larger context window (that can process more tokens at once).\n",
    "\n",
    "Currently, the largest context window model available is OpenAI's [GPT-4 turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo), which has a context limit of 128,000 tokens (about 90K words).\n",
    "\n",
    "But this is still limited. \n",
    "Many documents or collections of documents can easily exceed 100K tokens.\n",
    "\n",
    "How can we make this augmented approach more scalable?\n",
    "\n",
    "## Introducing: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "> Retrieval Augmented Generation: Break up the reference material (the lecture notes) into smaller chunks, then find a way to pull in the relevant chunks of context based on the query.\n",
    "\n",
    "In more detail:\n",
    "- In advance:\n",
    "    - Break up the reference material into chunks\n",
    "    - Create an embedding of each chunk\n",
    "    - Store these in an index\n",
    "- During prediction:\n",
    "    - Create an embedding of the query\n",
    "    - Find the most similar chunk of reference material to the query by comparing embeddings\n",
    "    - Pass the relevant chunk of reference material to the LLM along with the query to get an informed response\n",
    "\n",
    "Let's start off by splitting the reference material into sentences (much smaller chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by sentence (roughly)\n",
    "sentences = # TODO split the lecture notes by sentence\n",
    "\n",
    "# Print the lecture notes split by sentence (just the first 10)\n",
    "print ('\\n\\n--------- Sentence Break --------- \\n\\n'.join(sentences[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our lecture notes into numbers\n",
    "\n",
    "- We can represent a chunk of text as a point in space (anything from a single token to an entire body of text)\n",
    "- Similar words should be closer together\n",
    "- There are many pre-trained embedding models\n",
    "- One of them is available through the OpenAI API\n",
    "\n",
    "Here's a visualisation of what word embeddings look like:\n",
    "\n",
    "![](images/Word%20Embeddings.png)\n",
    "\n",
    "> Note: Anything can be turned into an embedding! Tokens, big chunks of text, images, you name it!\n",
    "\n",
    "## Using embeddings\n",
    "\n",
    "If you wanted to implement everything from scratch, here's what you would do:\n",
    "1. Pass each chunk of text through an embedding model to get the embedding\n",
    "1. Store those embeddings in a database\n",
    "1. Define a function that takes in an embedding (the query) and returns you the (top n) closest (as measured by cosine distance, for example) embeddings in your database (the reference material relevant to the query)\n",
    "\n",
    "Here, we're going to be a little more sophisticated, and use a pre-build embedding database.\n",
    "\n",
    "## Storing and Querying our Embeddings with ChromaDB\n",
    "\n",
    "[ChromaDB](https://www.trychroma.com/) is an open source software for storing and querying embeddings.\n",
    "\n",
    "Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can start using the embedding database. Check out the [documentation](https://docs.trychroma.com/getting-started) for the details of what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import chromadb\n",
    "\n",
    "# Initiallize a vector store to store our text and their respective embeddings\n",
    "chroma_client = # TODO create the chrome client\n",
    "vector_store = # TODO create a vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can add our chunks of text to the embedding database. \n",
    "\n",
    "When we do this, Chroma computes their embeddings behind the scenes, as described [here](https://docs.trychroma.com/embeddings#default-all-minilm-l6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our sentences into the vector store (this also creates their vector embeddings behind the scenes)\n",
    "# TODO add the sentences to the vector store\n",
    "        # TODO pass in the sentences\n",
    "        # TODO create a unique id for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the vector store is ready to be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying against our own lecture notes in the vector store to get the most similar sentences to our query\n",
    "# TODO query the vector store for the most similar sentences to the query\n",
    "    # TODO specify the query that we want to make\n",
    "    # TODO specify how many of the most similar sentences we want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraining our LLM with only lecture notes\n",
    "\n",
    "At this point, we've managed to get the chunks of text from our reference material that are most relevant to our query. Now, we need to put both of those things in a prompt to encourage the LLM to use that reference material in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a function called \"ask_query\" that takes in a query as an argument\n",
    "\n",
    "    # Get the most relevant sentences to our query\n",
    "    context =  # TODO query the vector store for the most similar 5 results to the query\n",
    "    context_list = # TODO get the most relevant sentences from the context (print to see what it looks like)\n",
    "    context_string = # TODO join the sentences together into one string\n",
    "\n",
    "    # Tell the LLM to only use the data we give it\n",
    "    guide_prompt = # TODO create a prompt that contains both the context and the query\n",
    "    messages = # TODO create a list of messages that ends with the quide prompt\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    # Give an output alongside sources\n",
    "    output = f'Answer:\\n\\n{response_content}\\n\\nSources:\\n\\n{context_string}'\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_query(\"What is an objective function?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this Jupyter Notebook, we explored the concept of Retrieval Augmented Generation (RAG) and how it can be used to limit the use of Language Model (LLM) to what they are best at. We started by reading a PDF file in Python and then used OpenAI's GPT-3 to ask questions about the lecture notes. We then introduced RAG and showed how it can be used to break up the reference material into smaller chunks and then find a way to pull in the relevant chunks of context based on the query. We used ChromaDB to store and query our embeddings and then constrained our LLM with only lecture notes. \n",
    "\n",
    "We hope this notebook has been helpful in understanding RAG and how it can be used to limit the use of LLMs to what they are best at. Feel free to modify and experiment with the code to see how it works with your own data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
