{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tame an LLM using Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this lecture:\n",
    "- The problem of LLM hallucination\n",
    "- Guide prompts that include reference material\n",
    "- Retrieval Augmented Generation (RAG)\n",
    "    - Embedding\n",
    "    - Vector stores\n",
    "    - Closest neighbour search\n",
    "\n",
    "### How do LLMs really work?\n",
    "- This is a useful thought experiment. ([Sidenote: This is where it all started](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "- Turns out they're fairly similar to us in some regards (long-term & short-term 'memory', ability to '[pay attention](https://arxiv.org/pdf/2307.03172.pdf)').\n",
    "- Where possible, put LLMs in position to use short-term memory and help them pay attention.\n",
    "- LLMs are great at [pattern matching](https://arxiv.org/abs/2005.14165) and following syntactic rules.\n",
    "- There are cases where LLMs provide a solution to a problem, but [they may be suboptimal](https://aclanthology.org/2023.findings-acl.426.pdf).\n",
    "\n",
    "### Retrieval Augmented Generation (RAG) - An Antidote to Hallucination\n",
    "- One such way of limiting the use of LLMs to what they are best at.\n",
    "\n",
    "- Uses [in-context learning](https://arxiv.org/abs/2301.00234) to give the LLM a usable short-term memory.\n",
    "\n",
    "Let's use this to ask questions about some lecture notes\n",
    "### Reading my PDF in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "reader = PdfReader(\"Lecture Notes.pdf\") # read the pdf file\n",
    "\n",
    "# Read each page and store them as a string\n",
    "lecture_notes  = ''.join([page.extract_text() for page in reader.pages]) # create one big string of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Load our OpenAI API key\n",
    "openai = OpenAI(api_key=os.getenv(\"OPEN_AI_API_KEY\")) # TODO set your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Generation - RAG's little brother\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An objective function is a criterion that is used to compare different designs in an optimization problem. It is expressed as a function of the design variables and represents the goal or objective that the optimization is trying to achieve. The objective function is typically specified based on physical or economic considerations and can vary depending on the specific problem. The objective function helps determine the best possible design by evaluating and comparing different designs based on the specified criteria.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_query (query, context):\n",
    "\n",
    "    # Tell the LLM to only use the data we give it\n",
    "    guide_prompt = # TODO create a guide prompt that asks the LLM to answer the query using the context\n",
    "    messages = # TODO create a list of messages that ends with the quide prompt\n",
    "\n",
    "    response = # TODO create a completion using the guide prompt\n",
    "\n",
    "    response_content = # TODO get the response content from the response\n",
    "\n",
    "    return(response_content)\n",
    "\n",
    "# TODO ask the LLM to answer the query using the lecture notes as context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if the lecture notes were twice as long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 7252 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/ice/lectures/workbench/projects/lecturing/lectures/A23 - Build Stuff with Generative AI/A23/2. Building AI Assistants Part 2 - Tools/Solutions 2.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ask_query(query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat is an objective function?\u001b[39;49m\u001b[39m\"\u001b[39;49m, context\u001b[39m=\u001b[39;49mlecture_notes\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32m/Users/ice/lectures/workbench/projects/lecturing/lectures/A23 - Build Stuff with Generative AI/A23/2. Building AI Assistants Part 2 - Tools/Solutions 2.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m guide_prompt \u001b[39m=\u001b[39m \u001b[39mfr\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39mUse only the following context to answer the query at the end: \u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mContext: \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m \u001b[39m# TODO create a guide prompt that asks the LLM to answer the query using the context\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m messages \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: guide_prompt}]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m response_content \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ice/lectures/workbench/projects/lecturing/lectures/A23%20-%20Build%20Stuff%20with%20Generative%20AI/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m(response_content)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/a23/lib/python3.12/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/a23/lib/python3.12/site-packages/openai/resources/chat/completions.py:556\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    514\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    555\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    558\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    559\u001b[0m             {\n\u001b[1;32m    560\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    561\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    562\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    563\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    564\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    565\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    566\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    568\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    569\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    570\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    571\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    572\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    573\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    574\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    575\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    576\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    577\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    578\u001b[0m             },\n\u001b[1;32m    579\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    580\u001b[0m         ),\n\u001b[1;32m    581\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    582\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    583\u001b[0m         ),\n\u001b[1;32m    584\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    585\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    586\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    587\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/a23/lib/python3.12/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/a23/lib/python3.12/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/a23/lib/python3.12/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 7252 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "ask_query(query=\"What is an objective function?\", context=lecture_notes*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we hit an issue: The context provided (the entire document of notes) is too large to be processed by the LLM.\n",
    "\n",
    "One solution might be to use an LLM with a larger context window (that can process more tokens at once).\n",
    "\n",
    "Currently, the largest context window model available is OpenAI's [GPT-4 turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo), which has a context limit of 128,000 tokens (about 90K words).\n",
    "\n",
    "But this is still limited. \n",
    "Many documents or collections of documents can easily exceed 100K tokens.\n",
    "\n",
    "How can we make this augmented approach more scalable?\n",
    "\n",
    "## Introducing: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "> Retrieval Augmented Generation: Break up the reference material (the lecture notes) into smaller chunks, then find a way to pull in the relevant chunks of context based on the query.\n",
    "\n",
    "In more detail:\n",
    "- In advance:\n",
    "    - Break up the reference material into chunks\n",
    "    - Create an embedding of each chunk\n",
    "    - Store these in an index\n",
    "- During prediction:\n",
    "    - Create an embedding of the query\n",
    "    - Find the most similar chunk of reference material to the query by comparing embeddings\n",
    "    - Pass the relevant chunk of reference material to the LLM along with the query to get an informed response\n",
    "\n",
    "Let's start off by splitting the reference material into sentences (much smaller chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 CHAPTER 1\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "INTRODUCTION\n",
      "1.1 Introduction\n",
      "Optimization is the act of achieving the best possible resul t under given circumstances.\n",
      "In design, construction, maintenance, ..., engineers have to take decisions\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "The goal of all\n",
      "such decisions is either to minimize eﬀort or to maximize bene ﬁt.\n",
      "The eﬀort or the beneﬁt can be usually expressed as a function o f certain design variables.\n",
      "Hence, optimization is the process of ﬁnding the conditions that give the maximum or the\n",
      "minimum value of a function.\n",
      "It is obvious that if a point x⋆corresponds to the minimum value of a function f(x), the\n",
      "same point corresponds to the maximum value of the function −f(x)\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "Thus, optimization\n",
      "can be taken to be minimization.\n",
      "Thereis nosinglemethodavailable for solvingall optimiza tion problemseﬃciently\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "Hence,\n",
      "a number of methods have been developed for solving diﬀerent t ypes of problems.\n",
      "Optimum seeking methods are also known as mathematical prog ramming techniques,\n",
      "which are a branch of operations research\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "Operations resea rch iscoarsely composed\n",
      "of the following areas.\n",
      "•Mathematical programming methods\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "These are useful in ﬁndi ng the minimum of a\n",
      "function of several variables under a prescribed set of cons traints.\n",
      "•Stochastic process techniques\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "These are used to analyze pr oblems which are de-\n",
      "scribed by a set of random variables of known distribution.\n",
      "•Statistical methods\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "These are used in the analysis of exper imental data and in the\n",
      "construction of empirical models.\n",
      "Theselecturenotesdealmainlywiththetheoryandapplicat ionsofmathematical program-\n",
      "mingmethods\n",
      "\n",
      "--------- Sentence Break --------- \n",
      "\n",
      "Mathematical programmingis a vast area of mat hematics and engineering.\n",
      "It includes\n",
      "•calculus of variations and optimal control;\n",
      "•linear, quadratic and non-linear programming;\n",
      "•geometric programming;\n",
      "•integer programming;\n",
      "•network methods (PERT);\n",
      "•game theory.\n",
      "The foundations of optimization can be traced back to Newton , Lagrange and Cauchy.\n",
      "The development of diﬀerential methods for optimization was possible because of the\n",
      "contribution of Newton and Leibnitz\n"
     ]
    }
   ],
   "source": [
    "# Split by sentence (roughly)\n",
    "sentences = # TODO split the lecture notes by sentence\n",
    "\n",
    "# Print the lecture notes split by sentence (just the first 10)\n",
    "print ('\\n\\n--------- Sentence Break --------- \\n\\n'.join(sentences[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our lecture notes into numbers\n",
    "\n",
    "- We can represent a chunk of text as a point in space (anything from a single token to an entire body of text)\n",
    "- Similar words should be closer together\n",
    "- There are many pre-trained embedding models\n",
    "- One of them is available through the OpenAI API\n",
    "\n",
    "Here's a visualisation of what word embeddings look like:\n",
    "\n",
    "![](images/Word%20Embeddings.png)\n",
    "\n",
    "> Note: Anything can be turned into an embedding! Tokens, big chunks of text, images, you name it!\n",
    "\n",
    "## Using embeddings\n",
    "\n",
    "If you wanted to implement everything from scratch, here's what you would do:\n",
    "1. Pass each chunk of text through an embedding model to get the embedding\n",
    "1. Store those embeddings in a database\n",
    "1. Define a function that takes in an embedding (the query) and returns you the (top n) closest (as measured by cosine distance, for example) embeddings in your database (the reference material relevant to the query)\n",
    "\n",
    "Here, we're going to be a little more sophisticated, and use a pre-build embedding database.\n",
    "\n",
    "## Storing and Querying our Embeddings with ChromaDB\n",
    "\n",
    "[ChromaDB](https://www.trychroma.com/) is an open source software for storing and querying embeddings.\n",
    "\n",
    "Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-0.4.15-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting requests>=2.28 (from chromadb)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from chromadb) (2.4.2)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Using cached chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.0.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from chromadb) (4.8.0)\n",
      "INFO: pip is looking at multiple versions of chromadb to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.4.14-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached chromadb-0.4.13-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached chromadb-0.4.12-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pydantic<2.0,>=1.9 (from chromadb)\n",
      "  Using cached pydantic-1.10.13-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.4.11-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached chromadb-0.4.10-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached chromadb-0.4.9-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting chroma-hnswlib==0.7.2 (from chromadb)\n",
      "  Using cached chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting chromadb\n",
      "  Using cached chromadb-0.4.8-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting numpy>=1.21.6 (from chromadb)\n",
      "  Using cached numpy-1.26.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "INFO: pip is still looking at multiple versions of chromadb to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.4.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached chromadb-0.4.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Using cached chromadb-0.4.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Using cached chromadb-0.4.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Using cached chromadb-0.4.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pandas>=1.3 (from chromadb)\n",
      "  Using cached pandas-2.1.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Collecting chroma-hnswlib==0.7.1 (from chromadb)\n",
      "  Using cached chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.4.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached chromadb-0.4.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached chromadb-0.4.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached chromadb-0.3.29-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting hnswlib>=0.7 (from chromadb)\n",
      "  Using cached hnswlib-0.7.0.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb)\n",
      "  Using cached clickhouse_connect-0.6.18-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting duckdb>=0.7.1 (from chromadb)\n",
      "  Using cached duckdb-0.9.1.tar.gz (10.6 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastapi==0.85.1 (from chromadb)\n",
      "  Using cached fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.3.27-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pydantic==1.9 (from chromadb)\n",
      "  Using cached pydantic-1.9.0-py3-none-any.whl (140 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.3.26-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Using cached chromadb-0.3.25-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Using cached chromadb-0.3.23-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting sentence-transformers>=2.2.2 (from chromadb)\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.7.22)\n",
      "Collecting urllib3>=1.26 (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pytz (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached zstandard-0.22.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached lz4-4.3.2.tar.gz (170 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.3->chromadb)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.10.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.28->chromadb)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=2.2.2->chromadb)\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: tqdm in /Users/ice/opt/miniconda3/envs/a23/lib/python3.12/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.66.1)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.3.22-py3-none-any.whl (69 kB)\n",
      "  Using cached chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
      "  Using cached chromadb-0.3.20-py3-none-any.whl (46 kB)\n",
      "  Using cached chromadb-0.3.18-py3-none-any.whl (46 kB)\n",
      "  Using cached chromadb-0.3.17-py3-none-any.whl (46 kB)\n",
      "  Using cached chromadb-0.3.16-py3-none-any.whl (46 kB)\n",
      "  Using cached chromadb-0.3.15-py3-none-any.whl (46 kB)\n",
      "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached chromadb-0.3.14-py3-none-any.whl (45 kB)\n",
      "  Using cached chromadb-0.3.13-py3-none-any.whl (45 kB)\n",
      "  Using cached chromadb-0.3.12-py3-none-any.whl (45 kB)\n",
      "  Using cached chromadb-0.3.11-py3-none-any.whl (41 kB)\n",
      "  Using cached chromadb-0.3.10-py3-none-any.whl (40 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached chromadb-0.3.8-py3-none-any.whl (40 kB)\n",
      "  Using cached chromadb-0.3.7-py3-none-any.whl (39 kB)\n",
      "  Using cached chromadb-0.3.6-py3-none-any.whl (39 kB)\n",
      "  Using cached chromadb-0.3.5-py3-none-any.whl (38 kB)\n",
      "  Using cached chromadb-0.3.4-py3-none-any.whl (38 kB)\n",
      "  Using cached chromadb-0.3.3-py3-none-any.whl (38 kB)\n",
      "  Using cached chromadb-0.3.2-py3-none-any.whl (37 kB)\n",
      "Collecting pandas~=1.3 (from chromadb)\n",
      "  Using cached pandas-1.5.3.tar.gz (5.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clickhouse-connect~=0.5.7 (from chromadb)\n",
      "  Using cached clickhouse-connect-0.5.25.tar.gz (70 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting duckdb~=0.5.1 (from chromadb)\n",
      "  Using cached duckdb-0.5.1.tar.gz (13.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastapi~=0.85.1 (from chromadb)\n",
      "  Downloading fastapi-0.85.2-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m983.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn~=0.18.3 (from uvicorn[standard]~=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting chromadb\n",
      "  Downloading chromadb-0.3.1-py3-none-any.whl (37 kB)\n",
      "  Downloading chromadb-0.3.0-py3-none-any.whl (36 kB)\n",
      "  Downloading chromadb-0.2.0-py3-none-any.whl (36 kB)\n",
      "Collecting uuid~=1.30 (from chromadb)\n",
      "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting chromadb\n",
      "  Downloading chromadb-0.1.0-py3-none-any.whl (34 kB)\n",
      "\u001b[31mERROR: Cannot install chromadb==0.1.0, chromadb==0.2.0, chromadb==0.3.0, chromadb==0.3.1, chromadb==0.3.2, chromadb==0.3.25, chromadb==0.3.26, chromadb==0.3.27, chromadb==0.3.29, chromadb==0.4.0, chromadb==0.4.1, chromadb==0.4.10, chromadb==0.4.11, chromadb==0.4.12, chromadb==0.4.13, chromadb==0.4.14, chromadb==0.4.15, chromadb==0.4.2, chromadb==0.4.3, chromadb==0.4.4, chromadb==0.4.5, chromadb==0.4.6, chromadb==0.4.7, chromadb==0.4.8 and chromadb==0.4.9 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    chromadb 0.4.15 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.14 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.13 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.12 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.11 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.10 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.9 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.8 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.7 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.6 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.5 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.4 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.3 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.2 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.1 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.4.0 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.3.29 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.3.27 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.3.26 depends on pulsar-client>=3.1.0\n",
      "    chromadb 0.3.25 depends on onnxruntime>=1.14.1\n",
      "    chromadb 0.3.2 depends on numpy~=1.21.6\n",
      "    chromadb 0.3.1 depends on numpy~=1.21.6\n",
      "    chromadb 0.3.0 depends on numpy~=1.21.6\n",
      "    chromadb 0.2.0 depends on numpy~=1.21.6\n",
      "    chromadb 0.1.0 depends on numpy~=1.21.6\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can start using the embedding database. Check out the [documentation](https://docs.trychroma.com/getting-started) for the details of what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO import chromadb\n",
    "\n",
    "# Initiallize a vector store to store our text and their respective embeddings\n",
    "chroma_client = # TODO create the chrome client\n",
    "vector_store = # TODO create a vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can add our chunks of text to the embedding database. \n",
    "\n",
    "When we do this, Chroma computes their embeddings behind the scenes, as described [here](https://docs.trychroma.com/embeddings#default-all-minilm-l6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our sentences into the vector store (this also creates their vector embeddings behind the scenes)\n",
    "# TODO add the sentences to the vector store\n",
    "        # TODO pass in the sentences\n",
    "        # TODO create a unique id for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the vector store is ready to be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TonyE\\OneDrive - Imperial College London\\Documents\\GitHub\\A23\\2. Building AI Assistants Part 2 - Tools\\Solutions 2.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TonyE/OneDrive%20-%20Imperial%20College%20London/Documents/GitHub/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Querying against our our own lecture notes in the vector store to get the most similar sentences to our query\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TonyE/OneDrive%20-%20Imperial%20College%20London/Documents/GitHub/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vector_store\u001b[39m.\u001b[39mquery(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TonyE/OneDrive%20-%20Imperial%20College%20London/Documents/GitHub/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     query_texts\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mWhat is an objective function?\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TonyE/OneDrive%20-%20Imperial%20College%20London/Documents/GitHub/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     n_results\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TonyE/OneDrive%20-%20Imperial%20College%20London/Documents/GitHub/A23/2.%20Building%20AI%20Assistants%20Part%202%20-%20Tools/Solutions%202.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "# Querying against our own lecture notes in the vector store to get the most similar sentences to our query\n",
    "# TODO query the vector store for the most similar sentences to the query\n",
    "    # TODO specify the query that we want to make\n",
    "    # TODO specify how many of the most similar sentences we want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraining our LLM with only lecture notes\n",
    "\n",
    "At this point, we've managed to get the chunks of text from our reference material that are most relevant to our query. Now, we need to put both of those things in a prompt to encourage the LLM to use that reference material in its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a function called \"ask_query\" that takes in a query as an argument\n",
    "\n",
    "    # Get the most relevant sentences to our query\n",
    "    context =  # TODO query the vector store for the most similar 5 results to the query\n",
    "    context_list = # TODO get the most relevant sentences from the context (print to see what it looks like)\n",
    "    context_string = # TODO join the sentences together into one string\n",
    "\n",
    "    # Tell the LLM to only use the data we give it\n",
    "    guide_prompt = # TODO create a prompt that contains both the context and the query\n",
    "    messages = # TODO create a list of messages that ends with the quide prompt\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "\n",
    "    # Give an output alongside sources\n",
    "    output = f'Answer:\\n\\n{response_content}\\n\\nSources:\\n\\n{context_string}'\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer\n",
      "\n",
      "An objective function is a function that is expressed in terms of design variables and represents the goal of either minimizing effort or maximizing benefit in decision-making. It is used in the process of optimization to find the conditions that result in the maximum or minimum value of the function.\n",
      "\n",
      "Sources:\n",
      "\n",
      "This criterion, wh en expressed as a function of\n",
      "the design variables, is known as objective function\n",
      "The goal of all\n",
      "such decisions is either to minimize eﬀort or to maximize bene ﬁt.\n",
      "The eﬀort or the beneﬁt can be usually expressed as a function o f certain design variables.\n",
      "Hence, optimization is the process of ﬁnding the conditions that give the maximum or the\n",
      "minimum value of a function.\n",
      "It is obvious that if a point x⋆corresponds to the minimum value of a function f(x), the\n",
      "same point corresponds to the maximum value of the function −f(x)\n",
      "Howeve r, the selection of an objective\n",
      "functionisnottrivial, becausewhatistheoptimal designw ithrespecttoacertaincriterion\n",
      "may be unacceptable with respect to another criterion\n",
      "Typi cally there is a trade oﬀ\n",
      "performance–cost, or performance–reliability, hence the selection of the objective function\n",
      "is one of the most important decisions in the whole design pro cess\n",
      "The set of all these\n",
      "surfaces are called objective function surfaces.\n",
      "Once the objective function surfaces are drawn, together wi th the constraint surfaces, the\n",
      "optimization problemcan beeasily solved, at least intheca seofatwo dimensionaldecision\n",
      "space, as shown in Figure 1.2\n"
     ]
    }
   ],
   "source": [
    "print(ask_query(\"What is an objective function?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this Jupyter Notebook, we explored the concept of Retrieval Augmented Generation (RAG) and how it can be used to limit the use of Language Model (LLM) to what they are best at. We started by reading a PDF file in Python and then used OpenAI's GPT-3 to ask questions about the lecture notes. We then introduced RAG and showed how it can be used to break up the reference material into smaller chunks and then find a way to pull in the relevant chunks of context based on the query. We used ChromaDB to store and query our embeddings and then constrained our LLM with only lecture notes. \n",
    "\n",
    "We hope this notebook has been helpful in understanding RAG and how it can be used to limit the use of LLMs to what they are best at. Feel free to modify and experiment with the code to see how it works with your own data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
